<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Are Models Biased on Text Without Gender-related Language?</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://d3js.org/d3.v4.js"></script>
    <!-- <script src="https://d3js.org/d3-scale-chromatic.v0.3.min.js"></script> -->
</head>
<body>
    <main>
        <div class="container-fluid py-5 jumbotron">
            <div class="container">
                <h1 class="display-4">Are Models Biased on Text Without Gender-related Language?</h1>
                <p class="lead fw-bold">Catarina Belem, Preethi Seshadri, Yasaman Razeghi, Sameer Singh</p>
                <p class="lead">Work accepted and presented at the main ICLR 2024 conference.</p>
                <div class="text-center">
                        <a class="btn btn-primary btn-lg" href="https://github.com/ucinlp/unstereo-eval" role="button">Dataset</a>
                        <a class="btn btn-primary btn-lg" href="https://openreview.net/forum?id=w1JanwReU6" role="button">View paper</a>
                        <a class="btn btn-primary btn-lg" href="https://github.com/ucinlp/unstereo-eval" role="button">View code</a>
                </div>
            </div>
        </div>
        <div class="container py-5">
            <div class="summary">
                <h2 class="text-primary">Abstract</h2>
                <p class="text-secondary">Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions.
A key assumption in prior works is that models reinforce stereotypes in the training data by picking up on gendered correlations.  In this paper, we challenge this assumption and instead address the question:
<span class="fw-bold">Do language models still exhibit gender bias in non-stereotypical settings?</span> To do so, we introduce <span class="text-primary fw-bold">UnStereoEval (USE)</span>, a novel framework tailored for investigating gender bias in stereotype-free scenarios.
USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language. 
By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of  stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words.
These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation.
                </p>
            </div>
        </div>
         

    <footer class="container-fluid py-5 pb-3 text-muted text-small text-center"><small>Copyright Â© 2024</small></footer>        
    </main>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js" integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js" integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF" crossorigin="anonymous"></script>
</body>
</html>
