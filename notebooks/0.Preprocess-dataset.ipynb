{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Data Loading: Load PMI difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1feeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GROUP_PAIRED_WORDLIST, FEMALE_WORDS, MALE_WORDS, get_pmi_diff, get_gender_pairs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792ee5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152515\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmi__her</th>\n",
       "      <th>pmi__his</th>\n",
       "      <th>pmi__him</th>\n",
       "      <th>pmi__hers</th>\n",
       "      <th>pmi__mother</th>\n",
       "      <th>pmi__father</th>\n",
       "      <th>pmi__mom</th>\n",
       "      <th>pmi__dad</th>\n",
       "      <th>pmi__mummy</th>\n",
       "      <th>pmi__daddy</th>\n",
       "      <th>...</th>\n",
       "      <th>pmi__queen</th>\n",
       "      <th>pmi__king</th>\n",
       "      <th>pmi__queens</th>\n",
       "      <th>pmi__kings</th>\n",
       "      <th>pmi__princess</th>\n",
       "      <th>pmi__prince</th>\n",
       "      <th>pmi__princesses</th>\n",
       "      <th>pmi__princes</th>\n",
       "      <th>pmi__he</th>\n",
       "      <th>pmi__she</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>80439.000000</td>\n",
       "      <td>98771.000000</td>\n",
       "      <td>65608.000000</td>\n",
       "      <td>7537.000000</td>\n",
       "      <td>30706.000000</td>\n",
       "      <td>29684.000000</td>\n",
       "      <td>10998.000000</td>\n",
       "      <td>10495.000000</td>\n",
       "      <td>1717.000000</td>\n",
       "      <td>2977.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10119.000000</td>\n",
       "      <td>19446.000000</td>\n",
       "      <td>3313.000000</td>\n",
       "      <td>6617.000000</td>\n",
       "      <td>5412.000000</td>\n",
       "      <td>8203.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>3825.000000</td>\n",
       "      <td>100828.000000</td>\n",
       "      <td>66891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-24.827642</td>\n",
       "      <td>-24.861843</td>\n",
       "      <td>-24.803681</td>\n",
       "      <td>-24.205257</td>\n",
       "      <td>-24.915487</td>\n",
       "      <td>-24.912725</td>\n",
       "      <td>-25.195694</td>\n",
       "      <td>-25.311220</td>\n",
       "      <td>-24.439117</td>\n",
       "      <td>-25.298108</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.361834</td>\n",
       "      <td>-25.395301</td>\n",
       "      <td>-24.835216</td>\n",
       "      <td>-24.729553</td>\n",
       "      <td>-25.019586</td>\n",
       "      <td>-25.328138</td>\n",
       "      <td>-23.698000</td>\n",
       "      <td>-23.932025</td>\n",
       "      <td>-25.416424</td>\n",
       "      <td>-25.262707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.563299</td>\n",
       "      <td>1.580690</td>\n",
       "      <td>1.506580</td>\n",
       "      <td>1.445382</td>\n",
       "      <td>1.324532</td>\n",
       "      <td>1.342077</td>\n",
       "      <td>1.403486</td>\n",
       "      <td>1.310585</td>\n",
       "      <td>1.538892</td>\n",
       "      <td>1.450787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435780</td>\n",
       "      <td>1.500407</td>\n",
       "      <td>1.723579</td>\n",
       "      <td>1.696653</td>\n",
       "      <td>1.477100</td>\n",
       "      <td>1.462137</td>\n",
       "      <td>1.718849</td>\n",
       "      <td>1.715749</td>\n",
       "      <td>1.514534</td>\n",
       "      <td>1.499545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-33.499275</td>\n",
       "      <td>-33.331670</td>\n",
       "      <td>-33.613325</td>\n",
       "      <td>-30.640672</td>\n",
       "      <td>-30.936595</td>\n",
       "      <td>-30.878193</td>\n",
       "      <td>-30.376505</td>\n",
       "      <td>-30.894450</td>\n",
       "      <td>-29.312282</td>\n",
       "      <td>-30.237593</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.485745</td>\n",
       "      <td>-31.237119</td>\n",
       "      <td>-28.636829</td>\n",
       "      <td>-29.793686</td>\n",
       "      <td>-30.302262</td>\n",
       "      <td>-31.337065</td>\n",
       "      <td>-30.067307</td>\n",
       "      <td>-29.995963</td>\n",
       "      <td>-32.885207</td>\n",
       "      <td>-33.474327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-25.685422</td>\n",
       "      <td>-25.682861</td>\n",
       "      <td>-25.638325</td>\n",
       "      <td>-25.184315</td>\n",
       "      <td>-25.755291</td>\n",
       "      <td>-25.742207</td>\n",
       "      <td>-26.175983</td>\n",
       "      <td>-26.204275</td>\n",
       "      <td>-25.419204</td>\n",
       "      <td>-26.245775</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.331394</td>\n",
       "      <td>-26.407824</td>\n",
       "      <td>-25.942235</td>\n",
       "      <td>-25.915272</td>\n",
       "      <td>-26.010199</td>\n",
       "      <td>-26.314027</td>\n",
       "      <td>-24.823475</td>\n",
       "      <td>-25.134534</td>\n",
       "      <td>-26.219694</td>\n",
       "      <td>-26.079852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-24.622905</td>\n",
       "      <td>-24.552552</td>\n",
       "      <td>-24.605622</td>\n",
       "      <td>-24.219955</td>\n",
       "      <td>-24.857188</td>\n",
       "      <td>-24.823301</td>\n",
       "      <td>-25.178166</td>\n",
       "      <td>-25.242244</td>\n",
       "      <td>-24.648881</td>\n",
       "      <td>-25.315466</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.429962</td>\n",
       "      <td>-25.440690</td>\n",
       "      <td>-25.287083</td>\n",
       "      <td>-24.966771</td>\n",
       "      <td>-25.088823</td>\n",
       "      <td>-25.347642</td>\n",
       "      <td>-24.085664</td>\n",
       "      <td>-24.178638</td>\n",
       "      <td>-25.137871</td>\n",
       "      <td>-25.078310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-23.743398</td>\n",
       "      <td>-23.767377</td>\n",
       "      <td>-23.746094</td>\n",
       "      <td>-23.240134</td>\n",
       "      <td>-24.040494</td>\n",
       "      <td>-24.000929</td>\n",
       "      <td>-24.242664</td>\n",
       "      <td>-24.399705</td>\n",
       "      <td>-23.807955</td>\n",
       "      <td>-24.462129</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.524708</td>\n",
       "      <td>-24.469532</td>\n",
       "      <td>-24.229567</td>\n",
       "      <td>-23.792893</td>\n",
       "      <td>-24.122755</td>\n",
       "      <td>-24.380551</td>\n",
       "      <td>-22.893234</td>\n",
       "      <td>-22.914827</td>\n",
       "      <td>-24.351869</td>\n",
       "      <td>-24.212292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-20.458226</td>\n",
       "      <td>-20.727246</td>\n",
       "      <td>-19.520618</td>\n",
       "      <td>-18.544282</td>\n",
       "      <td>-18.927156</td>\n",
       "      <td>-19.795987</td>\n",
       "      <td>-17.653614</td>\n",
       "      <td>-18.349328</td>\n",
       "      <td>-16.823430</td>\n",
       "      <td>-15.609870</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.835727</td>\n",
       "      <td>-18.720616</td>\n",
       "      <td>-16.770424</td>\n",
       "      <td>-17.806201</td>\n",
       "      <td>-17.986234</td>\n",
       "      <td>-18.367298</td>\n",
       "      <td>-15.972031</td>\n",
       "      <td>-16.816626</td>\n",
       "      <td>-20.899774</td>\n",
       "      <td>-21.228257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pmi__her      pmi__his      pmi__him    pmi__hers   pmi__mother  \\\n",
       "count  80439.000000  98771.000000  65608.000000  7537.000000  30706.000000   \n",
       "mean     -24.827642    -24.861843    -24.803681   -24.205257    -24.915487   \n",
       "std        1.563299      1.580690      1.506580     1.445382      1.324532   \n",
       "min      -33.499275    -33.331670    -33.613325   -30.640672    -30.936595   \n",
       "25%      -25.685422    -25.682861    -25.638325   -25.184315    -25.755291   \n",
       "50%      -24.622905    -24.552552    -24.605622   -24.219955    -24.857188   \n",
       "75%      -23.743398    -23.767377    -23.746094   -23.240134    -24.040494   \n",
       "max      -20.458226    -20.727246    -19.520618   -18.544282    -18.927156   \n",
       "\n",
       "        pmi__father      pmi__mom      pmi__dad   pmi__mummy   pmi__daddy  \\\n",
       "count  29684.000000  10998.000000  10495.000000  1717.000000  2977.000000   \n",
       "mean     -24.912725    -25.195694    -25.311220   -24.439117   -25.298108   \n",
       "std        1.342077      1.403486      1.310585     1.538892     1.450787   \n",
       "min      -30.878193    -30.376505    -30.894450   -29.312282   -30.237593   \n",
       "25%      -25.742207    -26.175983    -26.204275   -25.419204   -26.245775   \n",
       "50%      -24.823301    -25.178166    -25.242244   -24.648881   -25.315466   \n",
       "75%      -24.000929    -24.242664    -24.399705   -23.807955   -24.462129   \n",
       "max      -19.795987    -17.653614    -18.349328   -16.823430   -15.609870   \n",
       "\n",
       "       ...    pmi__queen     pmi__king  pmi__queens   pmi__kings  \\\n",
       "count  ...  10119.000000  19446.000000  3313.000000  6617.000000   \n",
       "mean   ...    -25.361834    -25.395301   -24.835216   -24.729553   \n",
       "std    ...      1.435780      1.500407     1.723579     1.696653   \n",
       "min    ...    -30.485745    -31.237119   -28.636829   -29.793686   \n",
       "25%    ...    -26.331394    -26.407824   -25.942235   -25.915272   \n",
       "50%    ...    -25.429962    -25.440690   -25.287083   -24.966771   \n",
       "75%    ...    -24.524708    -24.469532   -24.229567   -23.792893   \n",
       "max    ...    -17.835727    -18.720616   -16.770424   -17.806201   \n",
       "\n",
       "       pmi__princess  pmi__prince  pmi__princesses  pmi__princes  \\\n",
       "count    5412.000000  8203.000000      1266.000000   3825.000000   \n",
       "mean      -25.019586   -25.328138       -23.698000    -23.932025   \n",
       "std         1.477100     1.462137         1.718849      1.715749   \n",
       "min       -30.302262   -31.337065       -30.067307    -29.995963   \n",
       "25%       -26.010199   -26.314027       -24.823475    -25.134534   \n",
       "50%       -25.088823   -25.347642       -24.085664    -24.178638   \n",
       "75%       -24.122755   -24.380551       -22.893234    -22.914827   \n",
       "max       -17.986234   -18.367298       -15.972031    -16.816626   \n",
       "\n",
       "             pmi__he      pmi__she  \n",
       "count  100828.000000  66891.000000  \n",
       "mean      -25.416424    -25.262707  \n",
       "std         1.514534      1.499545  \n",
       "min       -32.885207    -33.474327  \n",
       "25%       -26.219694    -26.079852  \n",
       "50%       -25.137871    -25.078310  \n",
       "75%       -24.351869    -24.212292  \n",
       "max       -20.899774    -21.228257  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"../data\"\n",
    "\n",
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/pmi_by_gendered_expressions.csv\", index_col=0)\n",
    "print(len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fd3299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('she', 'he') pmi-defined words: 65912\n",
      "('her', 'his') pmi-defined words: 75032\n",
      "('her', 'him') pmi-defined words: 62131\n",
      "('hers', 'his') pmi-defined words: 7536\n",
      "! Pair (grandmother, grandfather) doesn't exist...\n",
      "! Pair (grandma, grandpa) doesn't exist...\n",
      "! Pair (stepmother, stepfather) doesn't exist...\n",
      "! Pair (stepmom, stepdad) doesn't exist...\n",
      "('mother', 'father') pmi-defined words: 26121\n",
      "('mom', 'dad') pmi-defined words: 9150\n",
      "('aunt', 'uncle') pmi-defined words: 5380\n",
      "! Pair (aunts, uncles) doesn't exist...\n",
      "('mummy', 'daddy') pmi-defined words: 1255\n",
      "('sister', 'brother') pmi-defined words: 15727\n",
      "('sisters', 'brothers') pmi-defined words: 8049\n",
      "('daughter', 'son') pmi-defined words: 18721\n",
      "('daughters', 'sons') pmi-defined words: 7276\n",
      "('female', 'male') pmi-defined words: 28115\n",
      "! Pair (females, males) doesn't exist...\n",
      "! Pair (feminine, masculine) doesn't exist...\n",
      "('woman', 'man') pmi-defined words: 31857\n",
      "('women', 'men') pmi-defined words: 38861\n",
      "! Pair (madam, sir) doesn't exist...\n",
      "! Pair (matriarchy, patriarchy) doesn't exist...\n",
      "('girl', 'boy') pmi-defined words: 21067\n",
      "! Pair (lass, lad) doesn't exist...\n",
      "('girls', 'boys') pmi-defined words: 18633\n",
      "('girlfriend', 'boyfriend') pmi-defined words: 5944\n",
      "('girlfriends', 'boyfriends') pmi-defined words: 1103\n",
      "('wife', 'husband') pmi-defined words: 20403\n",
      "('wives', 'husbands') pmi-defined words: 4507\n",
      "('queen', 'king') pmi-defined words: 9517\n",
      "('queens', 'kings') pmi-defined words: 2667\n",
      "('princess', 'prince') pmi-defined words: 4818\n",
      "('princesses', 'princes') pmi-defined words: 1094\n",
      "! Pair (lady, lord) doesn't exist...\n",
      "! Pair (ladies, lords) doesn't exist...\n",
      "('she', 'he') pmi-defined words: 65912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pmi__she</th>\n",
       "      <th>pmi__he</th>\n",
       "      <th>pmi_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149350</th>\n",
       "      <td>worldviews</td>\n",
       "      <td>-26.607044</td>\n",
       "      <td>-25.767380</td>\n",
       "      <td>-0.839664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97697</th>\n",
       "      <td>overspend</td>\n",
       "      <td>-25.071416</td>\n",
       "      <td>-24.994815</td>\n",
       "      <td>-0.076601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26858</th>\n",
       "      <td>caricaturing</td>\n",
       "      <td>-24.687485</td>\n",
       "      <td>-24.163941</td>\n",
       "      <td>-0.523544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123998</th>\n",
       "      <td>slatted</td>\n",
       "      <td>-25.064006</td>\n",
       "      <td>-25.430709</td>\n",
       "      <td>0.366702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>attentions</td>\n",
       "      <td>-23.186299</td>\n",
       "      <td>-23.783683</td>\n",
       "      <td>0.597384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110979</th>\n",
       "      <td>rearward</td>\n",
       "      <td>-27.267943</td>\n",
       "      <td>-26.689328</td>\n",
       "      <td>-0.578615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>12153</td>\n",
       "      <td>-24.776008</td>\n",
       "      <td>-24.636996</td>\n",
       "      <td>-0.139012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25189</th>\n",
       "      <td>burping</td>\n",
       "      <td>-23.977679</td>\n",
       "      <td>-24.346041</td>\n",
       "      <td>0.368362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116091</th>\n",
       "      <td>roadblocks</td>\n",
       "      <td>-25.271944</td>\n",
       "      <td>-24.999363</td>\n",
       "      <td>-0.272581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124507</th>\n",
       "      <td>smellin</td>\n",
       "      <td>-23.624592</td>\n",
       "      <td>-24.394020</td>\n",
       "      <td>0.769428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142425</th>\n",
       "      <td>unveils</td>\n",
       "      <td>-24.664684</td>\n",
       "      <td>-24.554712</td>\n",
       "      <td>-0.109972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115290</th>\n",
       "      <td>revised</td>\n",
       "      <td>-26.326665</td>\n",
       "      <td>-25.680263</td>\n",
       "      <td>-0.646403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50484</th>\n",
       "      <td>eternal</td>\n",
       "      <td>-25.227859</td>\n",
       "      <td>-24.657463</td>\n",
       "      <td>-0.570395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70243</th>\n",
       "      <td>incandescent</td>\n",
       "      <td>-25.456303</td>\n",
       "      <td>-25.545763</td>\n",
       "      <td>0.089460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84353</th>\n",
       "      <td>marriages</td>\n",
       "      <td>-24.964321</td>\n",
       "      <td>-25.226408</td>\n",
       "      <td>0.262087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word   pmi__she    pmi__he  pmi_diff\n",
       "149350    worldviews -26.607044 -25.767380 -0.839664\n",
       "97697      overspend -25.071416 -24.994815 -0.076601\n",
       "26858   caricaturing -24.687485 -24.163941 -0.523544\n",
       "123998       slatted -25.064006 -25.430709  0.366702\n",
       "16674     attentions -23.186299 -23.783683  0.597384\n",
       "110979      rearward -27.267943 -26.689328 -0.578615\n",
       "2268           12153 -24.776008 -24.636996 -0.139012\n",
       "25189        burping -23.977679 -24.346041  0.368362\n",
       "116091    roadblocks -25.271944 -24.999363 -0.272581\n",
       "124507       smellin -23.624592 -24.394020  0.769428\n",
       "142425       unveils -24.664684 -24.554712 -0.109972\n",
       "115290       revised -26.326665 -25.680263 -0.646403\n",
       "50484        eternal -25.227859 -24.657463 -0.570395\n",
       "70243   incandescent -25.456303 -25.545763  0.089460\n",
       "84353      marriages -24.964321 -25.226408  0.262087"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we may want to perform some correlation with other gendered words\n",
    "# we also define the PMI diff between words and other gendered word pairs\n",
    "GENDER_PAIRS, GENDER_PAIRS_NUM_WORDS = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "# ----------------------------------------------------------------------------\n",
    "# compute PMI diff used in the main paper\n",
    "# ----------------------------------------------------------------------------\n",
    "# Most analysis will focus on the pmi_diff(she, he)\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a049",
   "metadata": {},
   "source": [
    "## 2. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0969d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset names:\n",
      "  -> ['USE-5', 'USE-10', 'USE-20', 'Winobias-dev', 'Winobias-test', 'Winogender']\n"
     ]
    }
   ],
   "source": [
    "DATASET_2_FILEPATHS = {\n",
    "    \"USE-5\": \"../data/datasets/raw/USE-5.csv\",\n",
    "    \"USE-10\": \"../data/datasets/raw/USE-10.csv\",\n",
    "    \"USE-20\": \"../data/datasets/raw/USE-20.csv\",\n",
    "    \"Winobias-dev\": \"../data/datasets/raw/coref__Winobias__templates.dev.csv\",\n",
    "    \"Winobias-test\": \"../data/datasets/raw/coref__Winobias__templates.test.csv\",\n",
    "    \"Winogender\": \"../data/datasets/raw/coref__Winogender__templates.csv\",\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd74374",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "Transform the datasets into the canonic form:\n",
    "\n",
    "1. Transform model name into its canonic form: Extract from filepath name and add it as a column to the dataset.\n",
    "3. Obtain information about model size: \n",
    "2. Obtain information about the interventions: Is the model trained on duplicated data (is_deduped=False) or non-duplicated data (is_deduped=True).\n",
    "3. Obtain information about whether the test sentence pair is natural (is_natural=True) or whether is unnatural for one of the variants in the pair (is_natural=False)\n",
    "4. Obtain information about the model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f66cc9-e35e-409b-84de-36f7d317975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_unnatural_examples(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter out unnatural examples from the provided dataframe.\n",
    "    \n",
    "    Natural test sentence pairs are those for which ChatGPT\n",
    "    indicates that both sentence variants (regardless of gender)\n",
    "    are both likely to occur. If one of them is unlikely (as per\n",
    "    ChatGPT prediction) then we will deem the whole test sentence\n",
    "    pair unnatural and remove it.\n",
    "    \n",
    "    The proposed datasets were generated from scratch and therefore\n",
    "    will be the only ones with this column. The WinoBias and Winogender\n",
    "    have no such information, since we know by definition that both\n",
    "    completions of the sentences are both likely.\n",
    "    \"\"\"\n",
    "    if \"is_natural\" in df.columns:\n",
    "        return df[df[\"is_natural\"]].reset_index(drop=True)\n",
    "    elif \"likely_under\" in df.columns:\n",
    "        def is_likely_both(data: pd.Series) -> bool:\n",
    "            dct = eval(data)  # convert string to dict\n",
    "            return dct[\"male\"] == \"likely\" and dct[\"female\"] == \"likely\"\n",
    "            \n",
    "        return df[df[\"likely_under\"].apply(is_likely_both)].reset_index(drop=True)\n",
    "    else:\n",
    "        warnings.warn(f\"Dataset {df['dataset'].unique()} has no unnaturalness check... Skipping...\")\n",
    "        return df\n",
    "\n",
    "\n",
    "def read_filepath(fp: str, dataset: str, filter_unnatural: bool) -> pd.DataFrame:\n",
    "    print(\"\\n\\nReading path\", fp)\n",
    "    df = pd.read_csv(fp, index_col=0)    \n",
    "    df[\"dataset\"] = dataset\n",
    "\n",
    "    # add information about whether templates are likely or unlikely\n",
    "    if filter_unnatural:\n",
    "        print(\"\\t -> Found\", len(df), \"rows\")\n",
    "        df = remove_unnatural_examples(df)\n",
    "        print(\"\\t -> Remaining rows after filtering:\", len(df))\n",
    "   \n",
    "    df = df.reset_index(names=[\"orig_index\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c92a5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/USE-5.csv\n",
      "\t -> Found 4954 rows\n",
      "\t -> Remaining rows after filtering: 4405\n",
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/USE-10.csv\n",
      "\t -> Found 4943 rows\n",
      "\t -> Remaining rows after filtering: 4740\n",
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/USE-20.csv\n",
      "\t -> Found 4945 rows\n",
      "\t -> Remaining rows after filtering: 4839\n",
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/coref__Winobias__templates.dev.csv\n",
      "\t -> Found 792 rows\n",
      "\t -> Remaining rows after filtering: 792\n",
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/coref__Winobias__templates.test.csv\n",
      "\t -> Found 794 rows\n",
      "\t -> Remaining rows after filtering: 794\n",
      "\n",
      "\n",
      "Reading path ../data/datasets/raw/coref__Winogender__templates.csv\n",
      "\t -> Found 240 rows\n",
      "\t -> Remaining rows after filtering: 240\n",
      "\n",
      "\n",
      "-> Loaded 4405 sentences for dataset USE-5\n",
      "-> Loaded 4740 sentences for dataset USE-10\n",
      "-> Loaded 4839 sentences for dataset USE-20\n",
      "-> Loaded 792 sentences for dataset Winobias-dev\n",
      "-> Loaded 794 sentences for dataset Winobias-test\n",
      "-> Loaded 240 sentences for dataset Winogender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_600318/2719016084.py:27: UserWarning: Dataset ['Winobias-dev'] has no unnaturalness check... Skipping...\n",
      "  warnings.warn(f\"Dataset {df['dataset'].unique()} has no unnaturalness check... Skipping...\")\n",
      "/tmp/ipykernel_600318/2719016084.py:27: UserWarning: Dataset ['Winobias-test'] has no unnaturalness check... Skipping...\n",
      "  warnings.warn(f\"Dataset {df['dataset'].unique()} has no unnaturalness check... Skipping...\")\n",
      "/tmp/ipykernel_600318/2719016084.py:27: UserWarning: Dataset ['Winogender'] has no unnaturalness check... Skipping...\n",
      "  warnings.warn(f\"Dataset {df['dataset'].unique()} has no unnaturalness check... Skipping...\")\n"
     ]
    }
   ],
   "source": [
    "# Mapping from dataset name to the file dataframes\n",
    "DATASET_2_FILES = defaultdict(list)\n",
    "# Read each individual filepath, creating an association <str, <dataframe>>.\n",
    "\n",
    "# ------------------------------ ------------------------------ ------------------------------\n",
    "# To test the impact of ommiting the unnaturalness check, CHANGE THE VALUE BELOW TO FALSE\n",
    "FILTER_UNNATURAL = True\n",
    "# ------------------------------ ------------------------------ ------------------------------\n",
    "\n",
    "\n",
    "DATASET_2_FILES = {\n",
    "    dataset: read_filepath(fp, dataset, filter_unnatural=FILTER_UNNATURAL)\n",
    "    for dataset, fp in DATASET_2_FILEPATHS.items()\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "for dataset, df in DATASET_2_FILES.items():\n",
    "    print(f\"-> Loaded {len(df)} sentences for dataset {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776380d",
   "metadata": {},
   "source": [
    "## Post processing: \n",
    "\n",
    "In this section, we will carry some processing of the templates (column \"template\").\n",
    "\n",
    "\n",
    "**1. Remove placeholders from templates** : We first remove the placeholders (e.g., \"{PRONOUN}\", \"{PRONOUN1}\", \"{PRONOUN2}\", \"{PRONOUN2}self\") from the template.\n",
    "\n",
    "**2. Remove stopwords from the templates**: We use **spacy**'s stopwords except that we add back some of the pronouns, effectively following the approach in [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "\n",
    "**3. Parse each template**: We use **spacy** tokenizer since this was what was used by [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/). While NTLK is much faster, it doesn't group together words like \"self-care\", which is treated as single word by spacy tokenizer. Therefore, we've updated the script to consider the spacy tokenization. Applying it to the whole DATASET_2_FILES[dataset] will be too time-consuming, so we will apply to the first portion of the data and then concatenate it to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3857b",
   "metadata": {},
   "source": [
    "### Processing (using Spacy tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "349a8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cbelem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/cbelem/projects/tokenization-proj/venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "import nltk \n",
    "import re, string\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "PRONOUNS = [\"she\", \"her\", \"hers\", \"he\", \"his\", \"him\", \"himself\", \"herself\"]\n",
    "SPACY_PARSER = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"tagger\"])\n",
    "\n",
    "\n",
    "def postprocess_spacy(templates, pronouns=PRONOUNS):\n",
    "    def word_tokenize(sentence: str, pronouns: list, remove_stopwords: bool=True, remove_punct: bool=True):\n",
    "        doc = SPACY_PARSER(sentence)\n",
    "        # Extract the tokens that are not stopwords\n",
    "        tokens = [token.text for token in doc \n",
    "                  if (token.text in pronouns) or (not token.is_stop and not token.is_punct)]\n",
    "        return [t for t in tokens if len(t.strip()) > 0]\n",
    "\n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Step 1. Remove placeholders from the templates\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    # Step 2. Parse the sentence\n",
    "    templates = [word_tokenize(t, pronouns) for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "def postprocess_nltk(templates, pronouns=PRONOUNS):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    # We know that some sentences have some other references to other entities,\n",
    "    # let's keep some pronouns\n",
    "    nltk_stopwords -= set(pronouns)\n",
    "    punct = string.punctuation\n",
    "    \n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Remove pronouns first\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    \n",
    "    # Remove stopwords and punct\n",
    "    templates = [[w for w in word_tokenize(t) if w not in punct and w not in nltk_stopwords] for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "DATASET_2_TEMPLATES = {dataset: df[\"template\"].values.tolist() for dataset, df in DATASET_2_FILES.items()}\n",
    "DATASET_2_CANONIC_TEMPLATES_SPACY = {}\n",
    "DATASET_2_CANONIC_TEMPLATES_NLTK = {}\n",
    "\n",
    "for dataset, templates in DATASET_2_TEMPLATES.items():\n",
    "    DATASET_2_CANONIC_TEMPLATES_SPACY[dataset] = postprocess_spacy(templates)\n",
    "    DATASET_2_CANONIC_TEMPLATES_NLTK[dataset] = postprocess_nltk(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516756",
   "metadata": {},
   "source": [
    "## Determine gender co-occurrence for each word \n",
    "\n",
    "In this section, we iterate the templates and compute the gender co-occurrence values for each sentence in the benchmarks. Optionally, you can weight the values of each word by the likelihood of being toxic or having negative sentiment. If such values are not provided, we assume each word is worth the same value of 1 unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415b1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dataframe to mapping from word to pmi diff for easy access\n",
    "WORD2PMI = PMI_DIFF[[\"word\", \"pmi_diff\"]].set_index(\"word\").to_dict()[\"pmi_diff\"]\n",
    "WORD2WEIGHTS = defaultdict(lambda: 1)\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on frequency\n",
    "## ----------------------------------------------------------------\n",
    "FREQ_WORDS = pd.read_csv(\"../data/pmi_file_w_counts.csv\", index_col=0)\n",
    "FREQ_WORDS[\"log_freq\"] = np.log(FREQ_WORDS[\"freq\"])\n",
    "\n",
    "## uncomment one of the lines below if you prefer weighting each word based\n",
    "## on the frequency of each individual word\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"freq\"]].set_index(\"word\").to_dict()[\"freq\"]\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"log_freq\"]].set_index(\"word\").to_dict()[\"log_freq\"]\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on toxicity/sentiment\n",
    "## ----------------------------------------------------------------\n",
    "## TODO:\n",
    "## -> Define toxicity for each word\n",
    "## -> Define sentiment polarity for each word (?)\n",
    "## Define a 1-to-1 mapping and assign the variable WORD2WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06df4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi_diff_per_sentences(\n",
    "        templates: List[List[str]],\n",
    "        word2pmi: dict,\n",
    "        word2weights: dict,\n",
    "    ) -> List[List[float]]:\n",
    "    \"\"\"Computes the PMI difference per individual token in the provided sentences.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    It assumes the templates/sentences are already provided as a list of tokens.\n",
    "    It returns two lists: the first one contains the list of pmi values for each of\n",
    "    the provided words (some tokens won't have a PMI value associated); the second\n",
    "    list contains the 1-1 mapping from word to pmi value and their weights.\n",
    "    \"\"\"\n",
    "    pmi_values = []\n",
    "    words_with_pmi = []\n",
    "    \n",
    "    for template in templates:\n",
    "        pmi = np.array([word2weights[w] * word2pmi.get(w) for w in template if word2pmi.get(w) is not None])\n",
    "        pmiwords = [{\n",
    "            \"word\": w, \n",
    "            \"pmi\": round(word2pmi.get(w), 2),\n",
    "            \"weight\": round(word2weights[w], 2),\n",
    "        } for w in template if word2pmi.get(w) is not None]\n",
    "        \n",
    "        pmi_values.append(pmi)\n",
    "        words_with_pmi.append(pmiwords)\n",
    "            \n",
    "    return pmi_values, words_with_pmi\n",
    "        \n",
    "\n",
    "PMI_PER_SENTENCES_NLTK = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_NLTK.items()\n",
    "}\n",
    "\n",
    "PMI_PER_SENTENCES_SPACY = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_SPACY.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037bd55",
   "metadata": {},
   "source": [
    "Since in general **spacy** tokenizer leads to higher pct of examples being matched with a word. We will use the **spacy** tokenized templates to conduct the analysis (it increases the coverage of the constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_PER_TEMPLATES = {}\n",
    "PMIWORDS_PER_TEMPLATES = {}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Change the PMI_PER_SENTENCES_SPACY with PMI_PER_SENTENCES_NLTK\n",
    "# to use NLTK tokenization instead.\n",
    "# ------------------------------------------------------------------\n",
    "# for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_NLTK.items():\n",
    "for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_SPACY.items():\n",
    "    pmi_vals, words_per_pmi = pmi_per_sents_values\n",
    "    \n",
    "    PMI_PER_TEMPLATES[dataset] = pmi_vals\n",
    "    PMIWORDS_PER_TEMPLATES[dataset] = words_per_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6423c5",
   "metadata": {},
   "source": [
    "### Compute the constraint: MaxPMI(s)\n",
    "\n",
    "In this section, we compute the max gender PMI value per sentence. This consists of determining that's the max absolute word-level PMI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffa5b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "\n",
    "def max_gender_pmi(templates_pmi: List[List[str]], col: str) -> List[dict]:\n",
    "    \"\"\"Compute the maximum PMI diff per sentence.\"\"\"\n",
    "    def _max_pmi(lst_pmis: List[str]) -> float:\n",
    "        if len(lst_pmis) > 0:\n",
    "            idx = np.argmax(np.abs(lst_pmis))\n",
    "            return lst_pmis[idx]\n",
    "    \n",
    "    results = []\n",
    "    for template_pmi in templates_pmi:\n",
    "        max_val = _max_pmi(template_pmi)\n",
    "        results.append({col: max_val, f\"{col}_invalid\": max_val is None, \"template_words_pmi\": template_pmi})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4917c59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['orig_index', 'word', 'target_word', 'sentence', 'has_placeholder',\n",
       "       'template', 'modifications', 'likely_under', 'is_natural', 'has_word',\n",
       "       'is_revised', 'dataset', 'max_gender_pmi', 'max_gender_pmi_invalid',\n",
       "       'template_words_pmi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contains the max gender pmi values per sentence\n",
    "MAX_GENDER_PMI = {dataset: max_gender_pmi(templates_pmi, MAXGENDER_COL) \n",
    "                  for dataset, templates_pmi in PMI_PER_TEMPLATES.items()}\n",
    "\n",
    "MAX_GENDER_PMI_LONG = []\n",
    "for dataset, lst_value_dicts in MAX_GENDER_PMI.items():\n",
    "    for value_dict in lst_value_dicts:\n",
    "        r = {k: v for k, v in value_dict.items()}\n",
    "        r[\"dataset\"] = dataset\n",
    "        MAX_GENDER_PMI_LONG.append(r)\n",
    "\n",
    "MAX_GENDER_PMI_LONG = pd.DataFrame(MAX_GENDER_PMI_LONG)\n",
    "        \n",
    "# Adds the information to the original dataset with all models\n",
    "# originally, preserved in the variable DATASET_2_FILES\n",
    "DATASET_W_CONSTRAINTS = {dataset: pd.DataFrame(values) for dataset, values in MAX_GENDER_PMI.items()}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# \n",
    "#                        Dataset w/ MaxGender PMI constraint!\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "DATASET_W_CONSTRAINTS = {\n",
    "    dataset: pd.concat((DATASET_2_FILES[dataset], DATASET_W_CONSTRAINTS[dataset]), copy=True, axis=1)\n",
    "    for dataset in DATASET_NAMES\n",
    "}\n",
    "\n",
    "DATASET_W_CONSTRAINTS[DATASET_NAMES[0]].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477d976-d26b-4841-ac6c-98d6bb7b84d8",
   "metadata": {},
   "source": [
    "## Persist post-processed results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64da7349-ea89-43ce-ab97-644c576d6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df in DATASET_W_CONSTRAINTS.items():\n",
    "    df.to_csv(f\"../data/datasets/preprocessed/{dataset}-no-maxpmi-constraint.csv.gz\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
