{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1587b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools, warnings, pickle, time, os\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CAMERA-READY PLOTTING (thanks Alex Boyd!)\n",
    "# -----------------------------------------------------------------------\n",
    "# The following code is borrowed from material provided by Alex!\n",
    "FULL_WIDTH = 5.50107\n",
    "COL_WIDTH  = 4.50461\n",
    "\n",
    "\n",
    "# Put at top of plotting script (requires tex be installed though)\n",
    "matplotlib.rc('font', family='serif', size=20)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def adjust(fig, left=0.0, right=1.0, bottom=0.0, top=1.0, wspace=0.0, hspace=0.0):\n",
    "    fig.subplots_adjust(\n",
    "        left   = left,  # the left side of the subplots of the figure\n",
    "        right  = right,  # the right side of the subplots of the figure\n",
    "        bottom = bottom,  # the bottom of the subplots of the figure\n",
    "        top    = top,  # the top of the subplots of the figure\n",
    "        wspace = wspace,  # the amount of width reserved for blank space between subplots\n",
    "        hspace = hspace,  # the amount of height reserved for white space between subplots\n",
    "    )\n",
    "    \n",
    "\n",
    "def save_fig(fig, name, **kwargs):\n",
    "    fig.savefig(f\"./camera_ready/images/{name}.pdf\", bbox_inches=\"tight\", **kwargs)\n",
    "\n",
    "\n",
    "# Axes formatting\n",
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "\n",
    "\n",
    "# Accessibility\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "matplotlib.rcParams[\"axes.prop_cycle\"] = matplotlib.cycler(color=sns.color_palette(\"colorblind\"))\n",
    "\n",
    "\n",
    "# Composite plots \n",
    "def disable_axis(ax):\n",
    "    ax.set_zorder(-100)  # Avoids a visual rendering bug\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.setp(ax.spines.values(), color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3ff16",
   "metadata": {},
   "source": [
    "# Dataset generation - Stage 1.  Word Selection\n",
    "\n",
    "This notebook represents the very first step in our data generation pipeline. Since our goal is to create a dataset that is gender-invariant and free of gender co-occurring words, we will have to make sure that the words we use to bootstrap the generation of our dataset are, themselves, abiding by the properties we defined. In particular, we want to make sure that the words selected by our procedure satisfy the following property:\n",
    "\n",
    "$$ \\delta(w) = \\texttt{PMI}(w, \\texttt{\"she\"}) - \\texttt{PMI}(w, \\texttt{\"he\"}) \\in [-\\eta, \\eta] $$, where $w$ is a word in the vocabulary and $\\eta$ is a limit on how much skewed a word can be towards one of the gendered words. As detailed in the paper, we first compute the $\\texttt{PMI}$ values and then empirically bin the distribution in 20 symmetric bins around the origin.\n",
    "\n",
    "\n",
    "\n",
    "The notebook is organized as follows:\n",
    "1. Read the co-occurrence counts from PILE as well as the term-frequencies, as collected by [Razeghi et al (2022)](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "2. Preprocess the list of words to remove non-English words.\n",
    "3. Preprocess the remaining words and remove rare words (e.g., words in the 20% percentile).\n",
    "4. Compute the $\\delta(w) = \\texttt{PMI}(w, \\texttt{\"she\"}) - \\texttt{PMI}(w, \\texttt{\"he\"})$ value for every word $w$\n",
    "5. Sample a subset centered around the origin by sampling words that satisfy  $ -\\eta \\leq \\delta(w) \\leq \\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6670096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory where to find the files\n",
    "DATA_DIR = \"/extra/ucinlp1/cbelem/bias-dataset-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c104f80",
   "metadata": {},
   "source": [
    "## 1. Load term counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce49ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl_file(fp: str):\n",
    "    \"\"\"Wrapper to read pickled filepath.\"\"\"\n",
    "    print(\"Reading file at\", fp)\n",
    "    start = time.time()\n",
    "    with open(fp, 'rb') as tff:\n",
    "        data = pickle.load(tff)\n",
    "    end = time.time()\n",
    "    print(f\"Time to read file {(end-start)/60:.2f} min\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_original_coccurrence_files(parent_dir: str) -> dict: # 5GB\n",
    "    \"\"\"Wrapper to read the co-occurrence term count files from parent_dir.\"\"\"\n",
    "    return read_pkl_file(f\"{parent_dir}/all_co_words.pkl\")\n",
    "\n",
    "\n",
    "def read_original_tf_files(parent_dir) -> dict: # 16M\n",
    "    \"\"\"Wrapper to read the term-frequency file from parent_dir.\"\"\"\n",
    "    return read_pkl_file(f\"{parent_dir}/term_frequency.pkl\")\n",
    "\n",
    "\n",
    "def read_pmi_diff(filepath: str) -> pd.DataFrame: #1.9M\n",
    "    \"\"\"Read precomputed PMI difference file\"\"\"\n",
    "    # Read the PMI difference filepath\n",
    "    pmi_diff = {\"word\": [], \"pmi_diff\": []}\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        for row in f:\n",
    "            word, _, val = row.rpartition(\",\")\n",
    "            pmi_diff[\"word\"].append(word)\n",
    "            pmi_diff[\"pmi_diff\"].append(float(val))\n",
    "    return pd.DataFrame(pmi_diff).sort_values(\"pmi_diff\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54077ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to lookup for word -> term frequency\n",
    "TERM_COUNTS_DICT = read_original_tf_files(DATA_DIR)\n",
    "TERM_COUNTS_TOTAL = sum(TERM_COUNTS_DICT.values())\n",
    "# Convert term counts into dataframe to add more metadata\n",
    "TERM_COUNTS_DF = pd.DataFrame(TERM_COUNTS_DICT.items(), columns=[\"word\", \"counts\"])\n",
    "\n",
    "total_counts = sum(TERM_COUNTS_DICT.values())\n",
    "\n",
    "# Add a relative frequency column\n",
    "TERM_COUNTS_DF[\"freq\"] = TERM_COUNTS_DF[\"counts\"].apply(lambda x: x / total_counts)\n",
    "TERM_COUNTS_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058b312",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "**Keeping English Alphabet words:**\n",
    "In this section, we wish to exclude numbers, punctuation, non-english words from the list of words. Therefore, a first preprocessing step we do is to exclude any word that is not fully created based on the English alphabet. We use Python's default functionality `str.isalpha` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether the words belong to the english alphabet or not\n",
    "TERM_COUNTS_DF['isalpha'] = TERM_COUNTS_DF[\"word\"].apply(str.isalpha)\n",
    "\n",
    "english_alphabet = TERM_COUNTS_DF[\"isalpha\"].value_counts()[True]\n",
    "print(f'{english_alphabet/len(TERM_COUNTS_DF):.2%} of the examples',\n",
    "      f'(out of {len(TERM_COUNTS_DF)}) belong to the English alphabet.')\n",
    "\n",
    "# Drop words containing non-English alphabet characters\n",
    "TERM_COUNTS_DF = TERM_COUNTS_DF[TERM_COUNTS_DF[\"isalpha\"]]\n",
    "TERM_COUNTS_DF[TERM_COUNTS_DF[\"isalpha\"]].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5b1c5",
   "metadata": {},
   "source": [
    "**Removing non-English words**: However, restricting to the English alphabet does not exclude other languages. For example, the spanish word \"echaban\" would be kept if we only applied the previous procedure. One hypothesis would be to remove the non-English words, using a language detector. However, this may also remove borrowed foreign words that are common in the English language, like _influenza_. \n",
    "\n",
    "Therefore, in the following cells, we will use a heuristic approach that keeps a word in `TERM_COUNTS_DF` if one of the following conditions is satisfied:\n",
    "\n",
    "1. The [fasttext](https://fasttext.cc/docs/en/unsupervised-tutorial.html) character-level language classifier predicts the word language to be English word with at least `ENGLISH_PRED_THRESHOLD`% confidence.\n",
    "2. There exists a sense definition for word $w$ in [WordNet](https://wordnet.princeton.edu/).\n",
    "\n",
    "\n",
    "Note: We experimented with [langdetect](https://pypi.org/project/langdetect/) library from Google as well, but it performs poorly when identifying individual words, e.g., mentions that _hello_ is not English. Leading to a large number of false negatives (i.e., claiming English words are non-English words). On the other hand, fasttext proved to be much better at this task. Besides, it also gives the confidence associated with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b634502",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fasttext\n",
    "except:\n",
    "    # Install fasttext\n",
    "    !pip install fasttext\n",
    "    import fasttext\n",
    "from typing import List, Tuple \n",
    "\n",
    "    \n",
    "FTEXT_MODEL_NAME = \"lid.176.bin\"\n",
    "# Download the language detection model (trained w/ 176 languages)\n",
    "if not os.path.isfile(FTEXT_MODEL_NAME):\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "        \n",
    "# Load fasttext model\n",
    "FTEXT_MODEL = fasttext.load_model(FTEXT_MODEL_NAME)\n",
    "\n",
    "# Language threshold\n",
    "ENGLISH_PRED_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fccffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_predict(word: str, model):\n",
    "    \"\"\"Predicts the language using the specified fasttext model.\"\"\"\n",
    "    pred = model.predict(word, k=1)\n",
    "    return pred[0][0].replace(\"__label__\", \"\"), pred[1][0]\n",
    "\n",
    "\n",
    "# Determine whether words are english\n",
    "TERM_COUNTS_DF[\"ft_pred_lang\"], TERM_COUNTS_DF[\"ft_pred_conf\"] = zip(\n",
    "    *TERM_COUNTS_DF[\"word\"].apply(fasttext_predict, model=FTEXT_MODEL)\n",
    ")\n",
    "pred_eng_counts = TERM_COUNTS_DF[\"ft_pred_lang\"].value_counts()[\"en\"]\n",
    "print(f'{pred_eng_counts/len(TERM_COUNTS_DF):.2%} of the words are predicted to be English')\n",
    "print(f'Number of unique predicted languages: {TERM_COUNTS_DF[\"ft_pred_lang\"].nunique()}')\n",
    "TERM_COUNTS_DF[\"ft_pred_lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    # Install fasttext\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Count the number of wordnet senses\n",
    "TERM_COUNTS_DF[\"wordnet_counts\"] = TERM_COUNTS_DF[\"word\"].apply(lambda x: len(wordnet.synsets(x)))\n",
    "(TERM_COUNTS_DF[\"wordnet_counts\"].value_counts() / len(TERM_COUNTS_DF)).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae447f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_words_mask(df: pd.DataFrame, threshold) -> pd.DataFrame:\n",
    "    is_english = (df[\"ft_pred_lang\"] == \"en\") & (df[\"ft_pred_conf\"] > threshold)\n",
    "    is_in_wordnet = (is_english == False) & (df[\"wordnet_counts\"] > 0)\n",
    "    return (is_english | is_in_wordnet)\n",
    "\n",
    "\n",
    "IS_ENGLISH_MASK = english_words_mask(TERM_COUNTS_DF, ENGLISH_PRED_THRESHOLD)\n",
    "\n",
    "nonengl_terms_orig_pmi = TERM_COUNTS_DF[~IS_ENGLISH_MASK].sort_values(\"word\")\n",
    "print(\"Total number of non english words:\", len(nonengl_terms_orig_pmi))\n",
    "\n",
    "print(\"Examples of words dropped due to being dubbed not english according to our procedure...\")\n",
    "print(\"-\", \"\\n- \".join(nonengl_terms_orig_pmi[\"word\"].values[::2000]))\n",
    "\n",
    "TERM_COUNTS_DF = TERM_COUNTS_DF[IS_ENGLISH_MASK]\n",
    "len(TERM_COUNTS_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63a124",
   "metadata": {},
   "source": [
    "## Preprocess: Remove rare words\n",
    "\n",
    "Upon observation of the remaining words, we observe that some of the words in the list (e.g., \"succinylacetone\", \"clientage\") correspond to valid English words and, sometimes, typos. Since these words do not represent common English words, it could throw off our model during dataset generation (e.g., degrade generation, lack diversity). \n",
    "\n",
    "To account for this, we notice that these words are often rarer and occur less frequently in the dataset. As such, we decided to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25eb1d5-30cd-41f7-bf8b-9859ea8d0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13317ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the term counts vs rank of english words\n",
    "sns.lineplot(x=np.arange(len(TERM_COUNTS_DF)), y=TERM_COUNTS_DF[\"counts\"].values)\n",
    "plt.xscale(\"log\"); plt.xlabel(\"Term rank\")\n",
    "plt.yscale(\"log\"); plt.ylabel(\"Term counts\")\n",
    "\n",
    "q = 0.2\n",
    "q_val = TERM_COUNTS_DF[\"counts\"].quantile(q)\n",
    "plt.axhline(q_val, label=f\"{q:.0%} quantile: {q_val}\", ls=\"--\", c=\"r\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7a362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_freq_terms_alpha = TERM_COUNTS_DF[TERM_COUNTS_DF[\"counts\"] < q_val].sort_values(\"counts\", ascending=False)\n",
    "print(\"Total number of low freq words:\", len(low_freq_terms_alpha))\n",
    "\n",
    "print(\"Examples of words with higher rank (lower frequency):\")\n",
    "print(\"-\", \"\\n- \".join(low_freq_terms_alpha[\"word\"].values[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DF = TERM_COUNTS_DF[TERM_COUNTS_DF[\"counts\"] > q_val]\n",
    "TERM_COUNTS_DF.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676e551",
   "metadata": {},
   "source": [
    "`TERM_COUNTS_DF` contains the information about a good estimate the English terms that are likely to occur in the data. A few limitations that we should address in the future are:\n",
    "\n",
    "1. remove slang (e.g., making use of [SlangNet](https://aclanthology.org/L16-1686/), [SlangSD](http://liangwu.me/slangsd/), etc.\n",
    "2. remove names\n",
    "3. remove abbreviations\n",
    "\n",
    "\n",
    "For now, we will proceed, assuming this is the best subset of the English words we can derive from PILE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303118fa",
   "metadata": {},
   "source": [
    "## Computing the PMI for each word\n",
    "\n",
    "In this section, we will compute the $\\texttt{PMIDiff}(w)$ for every word $w$. To that end, we will first define a list of gendered words (eg, \"mother\", \"father\", \"boy\", \"girl\") and we will compute the $\\texttt{PMI}$ of every word and these _group words_. Note that the co-occurrence counts loaded in `TERMS_CO_OCCUR`, consist of counts within a window size 10 after stop words have been removed. These do not refer to co-occurrence counts within the same document. \n",
    "\n",
    "$$\\texttt{PMI}(w, g) = log \\frac{p(w, g)}{p(w)p(g)}$$, where $w$ is the word in the vocabullary and $g$ is a group word. PMI therefore represents the strength of association between the two words, namely, how likely are the two words to co-occur together when compared to appearing individually. A negative value indicates that the words are less likely to co-occur together, whereas a positive value implies that the words almost always appear together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This document is huge\n",
    "TERMS_CO_OCCUR = read_original_coccurrence_files(DATA_DIR)\n",
    "print(\"Number of bigrams:\", len(TERMS_CO_OCCUR))\n",
    "TERMS_CO_OCCUR_TOTAL = sum(TERMS_CO_OCCUR.values()) # 131M\n",
    "\n",
    "\n",
    "GROUP_TERMS = [\n",
    "    (\"she\", \"he\"),\n",
    "    (\"her\", \"his\"),\n",
    "    (\"her\", \"him\"),\n",
    "    (\"hers\", \"his\"),\n",
    "    (\"herself\", \"himself\"),\n",
    "    (\"grandmother\", \"grandfather\"),\n",
    "    (\"grandma\", \"grandpa\"),\n",
    "    (\"stepmother\", \"stepfather\"),\n",
    "    (\"stepmom\", \"stepdad\"),\n",
    "    (\"mother\", \"father\"),\n",
    "    (\"mom\", \"dad\"),\n",
    "    (\"aunt\", \"uncle\"),\n",
    "    (\"aunts\", \"uncles\"),\n",
    "    (\"mummy\", \"daddy\"),\n",
    "    (\"sister\", \"brother\"),\n",
    "    (\"sisters\", \"brothers\"),\n",
    "    (\"daughter\", \"son\"),\n",
    "    (\"daughters\", \"sons\"),\n",
    "    (\"female\", \"male\"),\n",
    "    (\"females\", \"males\"),\n",
    "    (\"feminine\", \"masculine\"),\n",
    "    (\"woman\", \"man\"),\n",
    "    (\"women\", \"men\"),\n",
    "    (\"madam\", \"sir\"),\n",
    "    (\"matriarchy\", \"patriarchy\"),\n",
    "    (\"girl\", \"boy\"),\n",
    "    (\"lass\", \"lad\"),\n",
    "    (\"girls\", \"boys\"),\n",
    "    (\"girlfriend\", \"boyfriend\"),\n",
    "    (\"girlfriends\", \"boyfriends\"),\n",
    "    (\"wife\", \"husband\"),\n",
    "    (\"wives\", \"husbands\"),\n",
    "    (\"queen\", \"king\"),\n",
    "    (\"queens\", \"kings\"),\n",
    "    (\"princess\", \"prince\"),\n",
    "    (\"princesses\", \"princes\"),\n",
    "    (\"lady\", \"lord\"),\n",
    "    (\"ladies\", \"lords\"),\n",
    "]\n",
    "FEMALE_TERMS, MALE_TERMS = zip(*GROUP_TERMS)\n",
    "\n",
    "ALL_TERMS = set(TERM_COUNTS_DF.word.values)\n",
    "ALL_TERMS.add(FEMALE_TERMS)\n",
    "ALL_TERMS.add(MALE_TERMS)\n",
    "\n",
    "# Since we're interested in computing the PMI value for every word and other K words\n",
    "# we will have to iterate it at least k times (which would be time consuming)\n",
    "# Therefore, we will filter out the structure to include only pairs where terms defined in \n",
    "# `terms` or group words appear.\n",
    "def select_subset(bigram_counts: dict, terms: set) -> dict:\n",
    "    results = {}\n",
    "    for bigram, counts in bigram_counts.items():\n",
    "        if bigram[0] in terms or bigram[1] in terms:\n",
    "            results[bigram] = counts       \n",
    "    return results\n",
    "\n",
    "\n",
    "# Update term counts dict to contain only the relevant terms\n",
    "TERM_COUNTS_DICT = {w: v for w, v in TERM_COUNTS_DICT.items() if w in ALL_TERMS}\n",
    "len(TERM_COUNTS_DICT), len(TERM_COUNTS_DF)\n",
    "\n",
    "# Update terms co-occurs\n",
    "TERMS_CO_OCCUR = select_subset(TERMS_CO_OCCUR, ALL_TERMS)\n",
    "print(\"Reduced number of bigrams:\", len(TERMS_CO_OCCUR)) # roughly 113M pairs remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi(unigram_counts: dict, bigram_counts: dict, w, g, unigram_total: int, bigram_total: int):\n",
    "    \"\"\"Compute PMI for a words w, g using the bigram and unigram counts structures.\"\"\"\n",
    "    p_w = unigram_counts.get(w, 0) / unigram_total\n",
    "    p_g = unigram_counts.get(g, 0) / unigram_total\n",
    "    \n",
    "    p_w_g = (bigram_counts.get((w, g), 0) + bigram_counts.get((g, w), 0)) / bigram_total\n",
    "    \n",
    "    if 0 in (p_w, p_g, p_w_g):\n",
    "        return None\n",
    "    \n",
    "    # For numerical stability, we opt for computing PMI as:\n",
    "    return np.log(p_w_g) - np.log(p_w) - np.log(p_g)\n",
    "\n",
    "\n",
    "def compute_pmi_per_group_word(words: List[str], group_words: List[str]):\n",
    "    results = defaultdict(list)\n",
    "    for group_word in set(group_words):\n",
    "        for word in words:\n",
    "            pmi = compute_pmi(\n",
    "                unigram_counts=TERM_COUNTS_DICT,\n",
    "                bigram_counts=TERMS_CO_OCCUR,\n",
    "                w=word, g=group_word,\n",
    "                unigram_total=TERM_COUNTS_TOTAL, \n",
    "                bigram_total=TERMS_CO_OCCUR_TOTAL)\n",
    "            \n",
    "            results[f\"pmi_{group_word}\"].append(pmi)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the PMI between every word and every female word\n",
    "PMI_FEMALE = compute_pmi_per_group_word(TERM_COUNTS_DF[\"word\"].values.tolist(), FEMALE_TERMS)\n",
    "\n",
    "# Compute the PMI between every word and every male word\n",
    "PMI_MALE = compute_pmi_per_group_word(TERM_COUNTS_DF[\"word\"].values.tolist(), MALE_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PMI_FEMALE), len(PMI_MALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc7f2f-f3fa-49e5-a565-a215a7ad1447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7b39d5",
   "metadata": {},
   "source": [
    "### Computing the PMI difference:\n",
    "\n",
    "To obtain a sense of how much more likely is a word to co-occur with female words than with male words, we can compute the difference of PMIs as follows:\n",
    "\n",
    "$$\\delta(w, g_F, g_M) = \\texttt{PMI}(w, g_F) - \\texttt{PMI}(w, g_M)$$, where $g_M$ and $g_F$ represent male and female gendered words, respectively.\n",
    "\n",
    "In the original version of this work, we simply determined the gendered co-occurrence of a word by computing $\\delta(w, \\texttt{\"she\"}, \\texttt{\"he\"})$. However, this may be suboptimal since many other words can be implicitly correlated with gender. \n",
    "In this notebook, we will compute the PMI difference as the $max_{(g_F, g_M) \\in (G_F, G_M)} |\\delta(w, g_F, g_M)|$, where $(G_F, G_M)$ is the list of paired group words (eg, as defined in `GROUP_TERMS`). The intuition is that we will represent the gender polarity of a word with the strongest existing correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# Every word w, we have len(GROUP_TERMS) PMI values\n",
    "# - some of which can be None, if one of the grouped words did not occur with w)\n",
    "results = defaultdict(list)\n",
    "results[\"word\"] = TERM_COUNTS_DF[\"word\"].values.tolist()\n",
    "\n",
    "for word_idx in range(len(TERM_COUNTS_DF)):\n",
    "    for fterm, mterm in GROUP_TERMS:\n",
    "        pmi_f = PMI_FEMALE[f\"pmi_{fterm}\"][word_idx]\n",
    "        pmi_m = PMI_MALE[f\"pmi_{mterm}\"][word_idx]\n",
    "        \n",
    "        # If one of the terms is not defined, append None\n",
    "        if pmi_f is None or pmi_m is None or math.isnan(pmi_f) or math.isnan(pmi_m):\n",
    "            results[f\"pmi_{fterm}_{mterm}\"].append(None)\n",
    "        else:\n",
    "            results[f\"pmi_{fterm}_{mterm}\"].append(pmi_f - pmi_m)\n",
    "            \n",
    "            \n",
    "results = pd.DataFrame(results)\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70cf941-05a7-413d-92bf-6071825d35e3",
   "metadata": {},
   "source": [
    "#### Marginal distribution of $\\delta(w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c9d10-eb9c-42b0-bc75-b7a9fe85c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals = pd.DataFrame({\n",
    "    \"pmi_value\": PMI_MALE[\"pmi_he\"] + PMI_FEMALE[\"pmi_she\"],\n",
    "    \"gender word\": [\"he\"] * len(PMI_MALE[\"pmi_he\"]) + [\"she\"] * len(PMI_FEMALE[\"pmi_she\"])\n",
    "})\n",
    "marginals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6d89e-f393-46da-ab88-3031a61ccd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(COL_WIDTH, COL_WIDTH))\n",
    "sns.histplot(marginals, x=\"pmi_value\", hue=\"gender word\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed41b41-07d7-4638-bc66-7da67bf0cde1",
   "metadata": {},
   "source": [
    "#### Joint distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f5604-9611-4507-9d93-15dd814657db",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"pmi_diff\": [f - m if f and m else None for f, m in zip(PMI_FEMALE[\"pmi_she\"], PMI_MALE[\"pmi_he\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4ceed-07f0-40ab-9fb4-f4e8eb5f074b",
   "metadata": {},
   "source": [
    "### Correlation matrix: Analysis of the correlation between different gendered word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_word_pairs = results.set_index(\"word\").copy()\n",
    "gendered_word_pairs.columns = [\"(\" + c.replace(\"pmi_\",\"\").replace(\"_\", \", \") + \")\" for c in gendered_word_pairs.columns]\n",
    "\n",
    "# Drop rows with no valid pmi diff\n",
    "subset_cols = sorted([c for c in gendered_word_pairs if gendered_word_pairs[c].isna().sum() < len(gendered_word_pairs)])\n",
    "gendered_word_pairs = gendered_word_pairs[subset_cols].corr(\"kendall\")\n",
    "\n",
    "\n",
    "matplotlib.rc('font', family='serif', size=8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH, FULL_WIDTH))\n",
    "sns.heatmap(gendered_word_pairs, \n",
    "            mask = np.triu(np.ones(gendered_word_pairs.shape)),\n",
    "            vmin=-1, vmax=1, center=0, cbar_kws={\"shrink\": 0.7},\n",
    "            cmap=\"seismic\", square=True, linewidths=0.05, ax=ax,\n",
    "           )\n",
    "\n",
    "# ax.set_xticks([])\n",
    "# ax.set_xticklabels([])\n",
    "# ax.tick_params(axis='x', labelrotation=33)\n",
    "adjust(fig)\n",
    "save_fig(fig, \"heatmap__alternative_pmi_definitions\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad6f61-2f1d-4849-973e-9819a0058dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_well_defined = len(results) - results.isna().sum(axis=0).copy()\n",
    "counts_well_defined = pd.DataFrame(counts_well_defined, columns=[\"Counts\"])\n",
    "counts_well_defined[\"fraction\"] = counts_well_defined[\"Counts\"] / len(results)\n",
    "print(counts_well_defined.sort_index().to_latex(float_format='{:0.2%}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a6bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a562a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee668f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee013b69",
   "metadata": {},
   "source": [
    "### Drop uncommon words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the most common words\n",
    "ORIG_PMI_DF[\"is_common\"] = ORIG_PMI_DF[\"word\"].isin(TERM_COUNTS_DF_ALPHA_UQ[\"word\"].values)\n",
    "ORIG_PMI_DF[\"is_common\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec646485",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_terms_orig_pmi = ORIG_PMI_DF[ORIG_PMI_DF[\"is_common\"] == False].sort_values(\"word\")\n",
    "print(\"Total number of low freq words:\", len(low_freq_terms_orig_pmi))\n",
    "\n",
    "print(\"Examples of words dropped due to lower frequency:\")\n",
    "print(\"-\", \"\\n- \".join(low_freq_terms_orig_pmi[\"word\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "sns.histplot(ORIG_PMI_DF, x=\"pmi_diff\", hue=\"is_common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ = ORIG_PMI_DF[ORIG_PMI_DF[\"is_common\"]].reset_index(drop=True)\n",
    "print(len(ORIG_PMI_DF), \"-->\", len(ORIG_PMI_DF_UQ), \"; delta =\", len(ORIG_PMI_DF)-len(ORIG_PMI_DF_UQ))\n",
    "ORIG_PMI_DF_UQ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ.drop(\"word\", axis=1).corr(\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_LANG[mask].sort_values(\"word\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7090095",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG = ORIG_PMI_DF_UQ_LANG[mask]\n",
    "ORIG_PMI_DF_UQ_ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(ORIG_PMI_DF_UQ_LANG, x=\"pred_conf\", y=\"wordnet_counts\", hue=\"is_english\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(ORIG_PMI_DF_UQ_LANG, x=\"pred_conf\", y=\"wordnet_counts\", hue=\"is_english\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23775eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(ORIG_PMI_DF_UQ_ENG, x=\"pred_conf\", y=\"pmi_diff\", hue=\"is_english\", s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "sns.jointplot(ORIG_PMI_DF_UQ_ENG, x=\"wordnet_counts\", y=\"pmi_diff\", s=5)\n",
    "plt.xlabel(\"Number of WordNet definitions\")\n",
    "plt.ylabel(\"PMI Difference, $\\delta(w)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f56df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c641e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG.shape[0], TERM_COUNTS_DF.shape[0], round(ORIG_PMI_DF_UQ_ENG.shape[0] / TERM_COUNTS_DF.shape[0], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd5e1b",
   "metadata": {},
   "source": [
    "### Obtain the words\n",
    "\n",
    "In the variable ORIG_PMI_DF_UQ_ENG, we have the selected English words.\n",
    "We have yet to reduce the set of words to the ones having the same root.\n",
    "Since we're using stratified sampling to select one word from each bin, we do not need to care too much about this. If two words with the same root are selected, it is likely that it is because they were sampled from different bins. In which case, it may suggest that there is a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "# define PMI range\n",
    "pmi_diff_max = ORIG_PMI_DF_UQ_ENG[\"pmi_diff\"].apply(np.abs).describe()[\"max\"]\n",
    "print(pmi_diff_max)\n",
    "\n",
    "pmi_diff_max = np.ceil(pmi_diff_max)\n",
    "bins = np.linspace(-pmi_diff_max, pmi_diff_max, num_bins)\n",
    "\n",
    "ORIG_PMI_DF_UQ_ENG.loc[:,\"pmi_diff_bins\"] = pd.cut(ORIG_PMI_DF_UQ_ENG[\"pmi_diff\"], bins)\n",
    "ORIG_PMI_DF_UQ_ENG[\"pmi_diff_bins\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94928f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = sorted(ORIG_PMI_DF_UQ_ENG[\"pmi_diff_bins\"].unique())\n",
    "interval_idx_middle = [ix for ix, interval in enumerate(intervals) if 0 in interval][0]\n",
    "intervals[interval_idx_middle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67986b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_bin = ORIG_PMI_DF_UQ_ENG[ORIG_PMI_DF_UQ_ENG[\"pmi_diff_bins\"] == intervals[interval_idx_middle]]\n",
    "sampling_bin = sampling_bin.sort_values(\"freq\", ascending=False)\n",
    "sampling_bin.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_bin.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d88697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_bin.sort_values(\"pmi_diff\").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_bin.sort_values(\"pmi_diff\").tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG[\"skews\"] = [\"male\"] * len(ORIG_PMI_DF_UQ_ENG)\n",
    "female_mask = ORIG_PMI_DF_UQ_ENG[\"pmi_diff\"] > 0\n",
    "ORIG_PMI_DF_UQ_ENG.loc[female_mask, \"skews\"] = \"female\"\n",
    "\n",
    "neutral_mask = (ORIG_PMI_DF_UQ_ENG[\"pmi_diff\"] >= -0.263) & (ORIG_PMI_DF_UQ_ENG[\"pmi_diff\"] <= 0.263)\n",
    "ORIG_PMI_DF_UQ_ENG.loc[neutral_mask, \"skews\"] = \"neutral\"\n",
    "\n",
    "ORIG_PMI_DF_UQ_ENG[\"skews\"].value_counts() / len(ORIG_PMI_DF_UQ_ENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb395a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_info(df: pd.DataFrame):\n",
    "    results = []\n",
    "    for ix, row in df.iterrows():\n",
    "        wordnet_defs = {}\n",
    "        \n",
    "        if row[\"wordnet_counts\"] > 0:\n",
    "            synsets = wordnet.synsets(row[\"word\"])\n",
    "            wordnet_defs = {s.name(): s.definition() for s in synsets}\n",
    "            \n",
    "        results.append(wordnet_defs)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "wordnet_sample = get_wordnet_info(ORIG_PMI_DF_UQ_ENG)\n",
    "ORIG_PMI_DF_UQ_ENG[\"wordnet_definitions\"] = wordnet_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG.to_csv(\"../results__pool_of_words_by_pmi.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG_NEUTRAL = ORIG_PMI_DF_UQ_ENG[ORIG_PMI_DF_UQ_ENG[\"skews\"] == \"neutral\"].copy()\n",
    "ORIG_PMI_DF_UQ_ENG_NEUTRAL.to_csv(\"../results__neutral__pool_of_words_by_pmi.csv\", index=None)\n",
    "len(ORIG_PMI_DF_UQ_ENG_NEUTRAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_DIR = \"\"\n",
    "SAMPLES = []\n",
    "for i, seed in enumerate((9123, 19223, 8172361, 91283, 72613)):\n",
    "    sample = ORIG_PMI_DF_UQ_ENG_NEUTRAL.sample(n=100, replace=False, random_state=seed)\n",
    "    \n",
    "    for num in (5, 10, 20):\n",
    "        os.makedirs(f\"../results-words{num}/words{i+1}\", exist_ok=True)\n",
    "        sample.to_csv(f\"../results-words{num}/words{i+1}/selected_words__{seed}.csv\")\n",
    "        words = sorted(sample[\"word\"].unique())\n",
    "\n",
    "        with open(f\"../results-words{num}/words{i+1}/words.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe70194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91797449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c310979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c959cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ORIG_PMI_DF_UQ_ENG.groupby('pmi_diff_bins', group_keys=False).apply(lambda x: x.sample(frac=0.005))\n",
    "sample[\"skews\"].value_counts() / len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"pmi_diff_bins\"].value_counts() / len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7aab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "sns.histplot(ORIG_PMI_DF_UQ_ENG, x=\"pmi_diff\", binwidth=0.1, ax=ax, label=f\"Original: {len(ORIG_PMI_DF_UQ_ENG)}\", stat=\"probability\")\n",
    "sns.histplot(sample, x=\"pmi_diff\", binwidth=0.1, ax=ax, label=f\"Sample: {len(sample)}\", stat=\"probability\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = ORIG_PMI_DF_UQ_ENG.groupby('pmi_diff_bins', group_keys=False).apply(lambda x: x.sample(min(len(x), 10), replace=False))\n",
    "sample2[\"skews\"].value_counts() / len(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "sns.histplot(ORIG_PMI_DF_UQ_ENG, x=\"pmi_diff\", binwidth=0.1, ax=ax, label=f\"Original: {len(ORIG_PMI_DF_UQ_ENG)}\", stat=\"probability\")\n",
    "sns.histplot(sample2, x=\"pmi_diff\", binwidth=0.1, ax=ax, label=f\"Sample: {len(sample2)}\", stat=\"probability\")\n",
    "plt.legend()\n",
    "sample2[\"pmi_diff_bins\"].value_counts() / len(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069703d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### add wordnet info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310973a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sample2.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7be892",
   "metadata": {},
   "source": [
    "### Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21628272",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"../results/selected_words.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef562f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DICT[\"he\"],TERM_COUNTS_DICT[\"his\"], TERM_COUNTS_DICT[\"him\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a598df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DICT[\"she\"], TERM_COUNTS_DICT[\"her\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6862c831",
   "metadata": {},
   "source": [
    "# takes roughly 5 min, and will require 30GB of RAM\n",
    "TERMS_CO_OCCUR = read_original_coccurrence_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a74db",
   "metadata": {},
   "source": [
    "In this file, we plan to select a set of words from the pretraining set in an automatic fashion. We'll try to make an intuitive choice by considering the following:\n",
    " \n",
    " $$\\text{PMI}(w, \\text{\"she\"}) - \\text{PMI}(w, \\text{\"he\"}) = log \\frac{P(\\text{\"she\"}|w)}{P(\\text{\"he\"}|w)}$$\n",
    "\n",
    "Thus, we will deem words whose odd ratio is 2.5 times smaller or larger to be unproprortionally skewed. We will not consider these words for our bias benchmark creation:\n",
    "- Remove words whose $\\frac{P(\\text{\"she\"}|w)}{P(\\text{\"he\"}|w)} \\geq \\tau \\vee \\frac{P(\\text{\"he\"}|w)}{P(\\text{\"she\"}|w)} \\geq \\tau$, where $\\tau = 2.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d2dcc",
   "metadata": {},
   "source": [
    "## Check original words frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv(\"../../experiments-tacl-june-2023/data/pmi_diffs_selected.csv\")\n",
    "# orig_df = orig_df[~orig_df[\"selected\"].isna()]\n",
    "\n",
    "orig_words_set = set(orig_df[\"word\"].unique())\n",
    "orig_df[\"is_common\"] = orig_df[\"word\"].isin(TERM_COUNTS_DF_ALPHA_UQ[\"word\"].values)\n",
    "orig_df[\"is_common\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50facf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=np.arange(len(TERM_COUNTS_DF_ALPHA)), y=TERM_COUNTS_DF_ALPHA[\"counts\"].values)\n",
    "\n",
    "idx = np.array(TERM_COUNTS_DF_ALPHA[TERM_COUNTS_DF_ALPHA[\"word\"].isin(orig_df[\"word\"])].index)\n",
    "sns.scatterplot(x=idx, y=TERM_COUNTS_DF_ALPHA[\"counts\"].values[idx], color=\"red\", s=15)\n",
    "plt.xscale(\"log\"); plt.xlabel(\"Term rank\")\n",
    "plt.yscale(\"log\"); plt.ylabel(\"Term counts\")\n",
    "\n",
    "q = 0.2\n",
    "q_val = TERM_COUNTS_DF_ALPHA[\"counts\"].quantile(q)\n",
    "plt.axhline(q_val, label=f\"{q:.0%} quantile: {q_val}\", ls=\"--\", c=\"r\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_df = pd.read_csv(\"../../experiments-aug-2023/results/selected_words.csv\")\n",
    "current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b145aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=np.arange(len(TERM_COUNTS_DF_ALPHA)), y=TERM_COUNTS_DF_ALPHA[\"counts\"].values)\n",
    "\n",
    "idx = np.array(TERM_COUNTS_DF_ALPHA[TERM_COUNTS_DF_ALPHA[\"word\"].isin(current_df[\"word\"])].index)\n",
    "sns.scatterplot(x=idx, y=TERM_COUNTS_DF_ALPHA[\"counts\"].values[idx], color=\"black\", s=15)\n",
    "plt.xscale(\"log\"); plt.xlabel(\"Term rank\")\n",
    "plt.yscale(\"log\"); plt.ylabel(\"Term counts\")\n",
    "\n",
    "q = 0.2\n",
    "q_val = TERM_COUNTS_DF_ALPHA[\"counts\"].quantile(q)\n",
    "plt.axhline(q_val, label=f\"{q:.0%} quantile: {q_val}\", ls=\"--\", c=\"r\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb867ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec225de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PMI_DF_UQ_ENG.sort_values(\"counts\", ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392f82b",
   "metadata": {},
   "source": [
    "## Originally picked words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2885c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_words = pd.concat((\n",
    "    pd.read_csv(\"../results-words5/words1/selected_words__9123.csv\", index_col=0),\n",
    "    pd.read_csv(\"../results-words5/words2/selected_words__19223.csv\", index_col=0),\n",
    "    pd.read_csv(\"../results-words5/words3/selected_words__8172361.csv\", index_col=0),\n",
    "    pd.read_csv(\"../results-words5/words4/selected_words__91283.csv\", index_col=0),\n",
    "    pd.read_csv(\"../results-words5/words5/selected_words__72613.csv\", index_col=0),\n",
    ")).drop_duplicates()\n",
    "print(len(orig_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_words.to_csv(\"selected_words.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3891acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_words[orig_words.word == \"whatcha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_COUNTS_DF[TERM_COUNTS_DF[\"wordnet_counts\"] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ed8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_words[orig_words.word.isin([\"votary\", \"wale\", \"waylaid\", \"waylay\", \"ween\", \"spasmodic\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea233611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
