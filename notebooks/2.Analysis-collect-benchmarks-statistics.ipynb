{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e4c1ef",
   "metadata": {},
   "source": [
    "# Benchmark Statistics\n",
    "\n",
    "In this notebook, we assess the evaluated benchmarks in terms of the length, average PMI diff, average Max Gender PMI diff in each sentence, number of gendered words, template length and position of the pronouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility constants used across evaluation notebooks\n",
    "from utils import GROUP_PAIRED_WORDLIST, FEMALE_WORDS, MALE_WORDS\n",
    "# Utility methods used across evaluation notebooks\n",
    "from utils import get_model_size, canonic_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32221a60",
   "metadata": {},
   "source": [
    "##### Load the word-level PMI \n",
    "\n",
    "The word-level PMI was pre computed from PILE it is computed based on the counts made available by [Razeghi et al. 2022](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "The file consists of precomputed pointwise mutual information (PMI) values for each word (row) and specific gendered words (as indicated in the column names, e.g., \"pmi_her\" defines the PMI value between every word and the word \"her\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"..\"\n",
    "\n",
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/word2gender_pmi_PILE.csv\", index_col=0)\n",
    "print(\"Length:\", len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5618c",
   "metadata": {},
   "source": [
    "#### PMI difference between gendered words\n",
    "\n",
    "Compute the PMI difference between col1 and col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_diff(\n",
    "    df: pd.DataFrame, \n",
    "    col1: str,\n",
    "    col2: str,\n",
    "    clip: int=None,\n",
    "    missing_val: float=0.0,\n",
    "    prefix_col: str=\"pmi__\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Obtains the PMI difference between columns col1 and col2. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "    \n",
    "    col1: str\n",
    "        The female word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "    \n",
    "    col2: str\n",
    "        The male word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "        \n",
    "    clip: int, optional\n",
    "        Positive integer, specifies the cap. If not specified, the pmi\n",
    "        difference is only computed for words that co-occur with both\n",
    "        (col1, col2). If specified, we will fill the PMI value with 0\n",
    "        (ideally it would be a very negative number). You can tweak\n",
    "        this value using 'missing_val'.\n",
    "    \n",
    "    missing_val: float, default 0\n",
    "        Default value used to replace values that are clipped.\n",
    "    \n",
    "    prefix_col: str\n",
    "        The prefix anteceding the col1 and col2 in the provided dataframe.\n",
    "        In our files, we prefixes all columns with gendered lexicons using\n",
    "        the \"pmi__\" prefix.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    To replicate the values of the paper you should pass female lexicon words\n",
    "    as col1 and male lexicon words as col2.\n",
    "    \"\"\"\n",
    "    assert f\"{prefix_col}{col1}\" in df.columns, f\"column {col1} is undefined in dataframe\"\n",
    "    assert f\"{prefix_col}{col2}\" in df.columns, f\"column {col2} is undefined in dataframe\"\n",
    "    \n",
    "    if clip is None:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].dropna()\n",
    "    else:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].fillna(missing_val)\n",
    "        \n",
    "    print(f\"('{col1}', '{col2}') pmi-defined words: {len(result)}\")\n",
    "    result[f\"pmi({col1})-pmi({col2})\"] = result[f\"{prefix_col}{col1}\"] - result[f\"{prefix_col}{col2}\"]\n",
    "    \n",
    "    if clip is not None:\n",
    "        result[f\"pmi({col1})-pmi({col2})\"].clip(lower=-clip, upper=clip, inplace=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_gender_pairs_matrix(\n",
    "    gender_pmi_df: pd.DataFrame,\n",
    "    parallel_terms: list,\n",
    "    **kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute the pmi difference between the pairs of parallel terms. \n",
    "    \n",
    "    Examples of parallel terms can be (she, he). In the gendered setting, it\n",
    "    expects the first term in the pair to refer to feminine and the\n",
    "    second term in the pair to be referring to masculine.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gender_pmi_df: pandas.DataFrame\n",
    "        The PMI of every word (row) and a specific word. \n",
    "        \n",
    "    parallel_terms: list of <str, str> pairs\n",
    "        List of gendered words whose PMI is present in 'gender_pmi_df'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Table with original PMI per word as well as the difference between\n",
    "        the specified words. Resulting columns will be named as \n",
    "        '{word1}-{word2}', where word1 and word2 are the first and second\n",
    "        words in the specified pairs.\n",
    "    \"\"\"\n",
    "    # dataframe with all the group pairs PMI (per word)\n",
    "    # (words for which no PMI diff is define)\n",
    "    pairs = gender_pmi_df[[\"word\"]].copy().set_index(\"word\")\n",
    "    num_words = []\n",
    "\n",
    "    for fword, mword in parallel_terms:\n",
    "        try:\n",
    "            # Compute the pmi difference between fword and mword\n",
    "            d = get_pmi_diff(gender_pmi_df, fword, mword, **kwargs).set_index(\"word\")\n",
    "            # Rename to be easier to visualize\n",
    "            d = d.rename({f\"pmi({fword})-pmi({mword})\": f\"{fword}-{mword}\"}, axis=1)\n",
    "            # Number of well-defined words for each of the gender pairs\n",
    "            num_words.append((f\"{fword}-{mword}\", len(d)))\n",
    "            pairs = pairs.join(d[[f\"{fword}-{mword}\"]])\n",
    "        except:\n",
    "            print(f\"Pair ({fword}, {mword}) doesn't exist...\")\n",
    "\n",
    "    return pairs, num_words\n",
    "\n",
    "\n",
    "# Since we may want to perform some correlation with other gendered words\n",
    "# we also define the PMI diff between words and other gendered word pairs\n",
    "GENDER_PAIRS, GENDER_PAIRS_NUM_WORDS = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# compute PMI diff used in the main paper\n",
    "# ----------------------------------------------------------------------------\n",
    "# Most analysis will focus on the pmi_diff(she, he)\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b509887",
   "metadata": {},
   "source": [
    "#### Read files\n",
    "\n",
    "Read the scores assigned to each test sentence pair for the proposed benchmarks: Ours-05, Ours-10, Ours-20, as well as the scores assigned to the sentence pairs in WinoBias and WinoGender. There should be 23 files for each benchmark (46 for Winobias, since there are dev and test files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"..\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"Ours-05\": glob.glob(f\"{BASE_DIR}/results-words5/final-results/*__scores__*.csv\"),\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winobias*__scores__*.csv\"),\n",
    "    \"Winogender\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winogender*__scores__*.csv\"),\n",
    "    # \"StereoSet\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*StereoSet*__scores__*.csv\"),\n",
    "    # We specify this order so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"Ours-10\": glob.glob(f\"{BASE_DIR}/results-words10/final-results/*__scores__*.csv\"),\n",
    "    \"Ours-20\": glob.glob(f\"{BASE_DIR}/results-words20/final-results/*__scores__*.csv\"),\n",
    "}\n",
    "DATASET_2_FILEPATHS = {k: sorted(v) for k, v in DATASET_2_FILEPATHS.items()}\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(DATASET_NAMES)\n",
    "\n",
    "for name, files in DATASET_2_FILEPATHS.items():\n",
    "    print(name, len(files), \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files paths\n",
    "# --------------------------------\n",
    "# When reading the filepaths, there are a few things we'd like to do\n",
    "# 1. record which model it belongs to\n",
    "def get_model_name(filepath: str, suffix=\"__scores__\") -> str:\n",
    "    \"\"\"This method assumes that the model name follows a given suffix\"\"\"\n",
    "    model_name = filepath.rpartition(suffix)[-1]\n",
    "    # remove the extension\n",
    "    model_name = model_name.rpartition(\".\")[0]\n",
    "    if model_name.startswith(\"__extra__ucinlp1__\"):\n",
    "        # print(model_name)\n",
    "        model_name = model_name.replace(\"__extra__ucinlp1__\",\"\").replace(\"__hf_models_\", \"\")\n",
    "        # print(model_name)\n",
    "    return model_name\n",
    "    \n",
    "# -----------------------------------------------------------------\n",
    "# For datasets containing multiple splits, separated across files\n",
    "# it will be the case, that we will have multiple model names for\n",
    "# the same dataset name.\n",
    "# -----------------------------------------------------------------\n",
    "# We will send a warning and merge the two files. Assuming\n",
    "# they are part of the same dataset. Please make sure that\n",
    "# the listed files are not redundant and that indeed can be\n",
    "# merged!\n",
    "# -----------------------------------------------------------------\n",
    "DATASET_2_FILES = defaultdict(list)\n",
    "for name, filepaths in DATASET_2_FILEPATHS.items():\n",
    "    models = {fp: get_model_name(fp) for fp in filepaths}\n",
    "    models_2_fp, models_2_data = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    for fp, model_name in models.items():\n",
    "        models_2_data[model_name].append(pd.read_csv(fp, index_col=0))\n",
    "        models_2_fp[model_name].append(fp)\n",
    "    \n",
    "    for model_name, dfs in models_2_data.items():\n",
    "        if len(dfs) > 1:\n",
    "            # print()\n",
    "            # print(f\"Dataset '{name}' contains more than one filepath per model. {models_2_fp[model_name]}\")\n",
    "            dfs_lens = [len(d) for d in dfs]\n",
    "            dfs = pd.concat(dfs).reset_index(drop=True)\n",
    "            assert len(dfs) == sum(dfs_lens), \"Invalid result when merging dataframes\"\n",
    "        else:\n",
    "            dfs = dfs[0]\n",
    "                \n",
    "        dfs[\"dataset\"] = name\n",
    "        dfs[\"is_deduped\"] = model_name.endswith(\"deduped\")\n",
    "        dfs[\"__model\"] = dfs[\"model\"].apply(lambda x: x.replace(\"__extra__ucinlp1__\", \"\").replace(\"__hf_models_\", \"\"))\n",
    "        dfs[\"model\"] = dfs[\"__model\"].apply(canonic_model_name)\n",
    "        dfs[\"model_basename\"] = dfs[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "\n",
    "        dfs[\"__model_size\"] = dfs[\"model\"].apply(get_model_size)\n",
    "        DATASET_2_FILES[name].append(dfs)\n",
    "\n",
    "DATASET_2_FILES = {k: pd.concat(v) for k, v in DATASET_2_FILES.items()}\n",
    "# Filter models by a single file\n",
    "DATASET_2_FILES = {k: df[df[\"__model\"] == \"EleutherAI__gpt-j-6b\"].reset_index(drop=True) for k, df in DATASET_2_FILES.items()}\n",
    "\n",
    "# comment section below to obtain results w/o likely/unliley\n",
    "# filter the results by the \"natural examples\"\n",
    "for dataset in DATASET_2_FILES:\n",
    "    df = DATASET_2_FILES[dataset]\n",
    "    if \"is_natural\" in df.columns:\n",
    "        DATASET_2_FILES[dataset] = df[df[\"is_natural\"]].reset_index(drop=True)\n",
    "        print(dataset, len(df), len(DATASET_2_FILES[dataset]))\n",
    "    else:\n",
    "        print(dataset, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the templates\n",
    "DATANAME_TO_TEMPLATES = {k: v[\"template\"].values.tolist() for k, v in DATASET_2_FILES.items()}\n",
    "\n",
    "# list the names of the datasets in our analysis\n",
    "DATANAMES = list(DATANAME_TO_TEMPLATES.keys())\n",
    "print(\"Considering the following for the analysis\", DATANAMES)\n",
    "DATASET_2_FILES[\"Ours-05\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49549057",
   "metadata": {},
   "source": [
    "## Compute statistics of benchmarks\n",
    "\n",
    "We'd like to compare different properties of the evaluated benchmarks. Namely, we'd like to compare the positions of the pronouns, their length, the diversity of words, etc. To perform this analysis, we will work on a template level (the sentence with the placeholder mask) of each benchmark. Then, we will transform them into their canonic form according to the following rules:\n",
    "\n",
    "1. Remove pronoun placeholder, since we do not want it to be mapped to any PMI word;\n",
    "2. Lowercase the templates;\n",
    "3. Remove stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string, re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "# We know that some sentences have some other references to other entities,\n",
    "# let's keep some pronouns\n",
    "print(len(NLTK_STOPWORDS))\n",
    "NLTK_STOPWORDS -= set([\"she\", \"her\", \"hers\", \"he\", \"his\", \"him\"])\n",
    "print(len(NLTK_STOPWORDS))\n",
    "\n",
    "PUNCT = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "TEMPLATE_LENGTH = defaultdict(list)\n",
    "\n",
    "for dataset in DATANAME_TO_TEMPLATES.keys():\n",
    "    # lower case\n",
    "    templates = [t.lower() for t in DATANAME_TO_TEMPLATES[dataset]]\n",
    "    # rename the reflexive pronoun to be matched by the regex below\n",
    "    templates = [re.sub(r\"\\{pronoun2\\}self\", \"{pronoun2}\", t) for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun[0-2]{0,1}?\\}\", \" PRONOUN \", t) for t in templates]\n",
    "    \n",
    "    # compute number of words (do not consider punctuation)\n",
    "    num_words = [len(word_tokenize(re.sub(\"PRONOUN\", \"\", t))) for t in templates]\n",
    "    # compute number of pronouns based on the placeholder masks. This is because\n",
    "    # the non placeholder pronouns will be already counted by the gender lexicon computation\n",
    "    pronouns = [re.findall(r\"PRONOUN\", t) for t in templates]\n",
    "    \n",
    "    TEMPLATE_LENGTH[\"dataset\"].extend([dataset] * len(templates))\n",
    "    TEMPLATE_LENGTH[\"num_words\"].extend(num_words)\n",
    "    TEMPLATE_LENGTH[\"num_pronouns\"].extend([len(p) for p in pronouns])\n",
    "    \n",
    "    for t, ps in zip(templates, pronouns):\n",
    "        \n",
    "        # Replace all punctuation with an empty string\n",
    "        t = t.translate(str.maketrans('', '', string.punctuation)) \n",
    "        t_words = t.split()\n",
    "        pronoun_indices = [ix for ix, w in enumerate(t_words) if w in ps]\n",
    "        if len(pronoun_indices) == 0:\n",
    "            print(dataset, t_words)\n",
    "        TEMPLATE_LENGTH[\"pronoun_positions\"].append(pronoun_indices)\n",
    "        TEMPLATE_LENGTH[\"first_pos\"].append(pronoun_indices[0])\n",
    "        TEMPLATE_LENGTH[\"last_pos\"].append(pronoun_indices[-1])\n",
    "        TEMPLATE_LENGTH[\"avg_pronoun_pos_in_sentence\"].append(np.mean(pronoun_indices))\n",
    "        \n",
    "\n",
    "TEMPLATE_LENGTH = pd.DataFrame(TEMPLATE_LENGTH)\n",
    "TEMPLATE_PROPERTIES = [\"dataset\", \"num_words\", \"num_pronouns\", \"first_pos\", \"last_pos\", \"avg_pronoun_pos_in_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median values for each property by dataset\")\n",
    "TEMPLATE_LENGTH[TEMPLATE_PROPERTIES].groupby(\"dataset\").median().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max values for each property by dataset\")\n",
    "TEMPLATE_LENGTH[TEMPLATE_PROPERTIES].groupby(\"dataset\").max().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonic templates\n",
    "DATANAME_TO_TEMPLATES_CANONIC: Dict[str, List[str]] = {k: v.copy() for k, v in DATANAME_TO_TEMPLATES.items()}\n",
    "\n",
    "for dataset in DATANAME_TO_TEMPLATES_CANONIC.keys():\n",
    "    # Lower case\n",
    "    templates = [t.lower() for t in DATANAME_TO_TEMPLATES_CANONIC[dataset]]\n",
    "    \n",
    "    # Remove pronouns first\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    \n",
    "    # Remove stopwords and punct\n",
    "    templates = [[w for w in word_tokenize(t) if w not in PUNCT and w not in NLTK_STOPWORDS] for t in templates]\n",
    "    \n",
    "    DATANAME_TO_TEMPLATES_CANONIC[dataset] = templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f06c01",
   "metadata": {},
   "source": [
    "## Analysis 1: Do generated sentences contain gendered language? \n",
    "\n",
    "Gendered language is language whose gender is explicitly marked in the word. For example, the words \"mother\", \"her\", \"woman\", \"unwomanly\" are all gendered words as they are associated with a specific gender.\n",
    "\n",
    "In this section, we exploit `DATANAME_TO_TEMPLATES_CANONIC` and adopt a bag of words approach to count the number of gendered expressions (or lexicon) occurring in each benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd736f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMALE_WORDS and MALE_WORDS are the words whose PMI values are precomputed.\n",
    "# FEMALE_LEXICON and MALE_LEXICON represent a larger set of words encompassing a wider range of gendered expressions.\n",
    "FEMALE_LEXICON = list(FEMALE_WORDS)\n",
    "MALE_LEXICON = list(MALE_WORDS)\n",
    "\n",
    "# Create directory to place the wordlists used in this study\n",
    "!mkdir gender-wordlist\n",
    "\n",
    "# ------------------------------------------------\n",
    "# BIAS BENCH \n",
    "# ------------------------------------------------\n",
    "## Obtain the gender-wordlist used in the BIASBENCH paper: https://arxiv.org/pdf/2110.08527.pdf\n",
    "!wget -P gender-wordlist https://raw.githubusercontent.com/McGill-NLP/bias-bench/main/data/bias_attribute_words.json\n",
    "\n",
    "import json\n",
    "with open(\"gender-wordlist/bias_attribute_words.json\") as f:\n",
    "    BB_GENDER_PAIR = json.load(f)[\"gender\"]\n",
    "    BB_GENDER_PAIR_M, BB_GENDER_PAIR_F = zip(*BB_GENDER_PAIR)\n",
    "print(\"BIASBench:\", len(BB_GENDER_PAIR)) \n",
    "\n",
    "FEMALE_LEXICON += BB_GENDER_PAIR_F\n",
    "MALE_LEXICON += BB_GENDER_PAIR_M\n",
    "\n",
    "# ------------------------------------------------\n",
    "# NAMES BENCH \n",
    "# ------------------------------------------------\n",
    "# based on https://github.com/McGill-NLP/bias-bench/blob/main/data/seat/angry_black_woman_stereotype.jsonl\n",
    "FEMALE_LEXICON += [\"Allison\",\"Anne\",\"Carrie\",\"Emily\",\"Jill\",\"Laurie\",\"Kristen\",\"Meredith\",\"Molly\",\"Amy\",\"Claire\",\"Katie\",\"Madeline\",\"Katelyn\",\"Emma\",\"Aisha\",\"Ebony\",\"Keisha\",\"Latonya\",\"Lakisha\",\"Latoya\",\"Tamika\",\"Imani\",\"Shanice\",\"Aaliyah\",\"Precious\",\"Nia\",\"Deja\",\"Latanya\",\"Latisha\"]\n",
    "\n",
    "# Based on https://github.com/McGill-NLP/bias-bench/blob/main/data/seat/weat6.jsonl\n",
    "FEMALE_LEXICON += [\"Amy\",\"Joan\",\"Lisa\",\"Sarah\",\"Diana\",\"Kate\",\"Ann\",\"Donna\"]\n",
    "MALE_LEXICON += [ \"John\", \"Paul\",\"Mike\",\"Kevin\",\"Steve\",\"Greg\",\"Jeff\",\"Bill\"]\n",
    "\n",
    "# Based on https://github.com/McGill-NLP/bias-bench/blob/main/data/seat/weat6b.jsonl\n",
    "FEMALE_LEXICON += [  \"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
    "MALE_LEXICON += [  \"male\",\"man\",\"boy\",\"brother\",\"he\",\"him\",\"his\",\"son\"]\n",
    "\n",
    "# Based on https://github.com/McGill-NLP/bias-bench/blob/main/data/seat/weat8.jsonl\n",
    "FEMALE_LEXICON += [\"sister\",\"mother\",\"aunt\",\"grandmother\",\"daughter\",\"she\",\"hers\",\"her\"]\n",
    "MALE_LEXICON += [\"brother\",\"father\",\"uncle\",\"grandfather\",\"son\",\"he\",\"his\",\"him\"]\n",
    "\n",
    "# Others\n",
    "FEMALE_LEXICON += [\"granddaughter\",\"granddaughters\"]\n",
    "MALE_LEXICON += [\"grandson\",\"grandsons\"]\n",
    "\n",
    "# Names based on wikipedia (?)\n",
    "# https://en.wikipedia.org/wiki/Category:English_masculine_given_names\n",
    "MALE_LEXICON += [\"brad\", \"cyrus\"]\n",
    "\n",
    "# UNIQUE\n",
    "FEMALE_LEXICON = sorted(set([w.lower() for w in FEMALE_LEXICON]))\n",
    "MALE_LEXICON = sorted(set([w.lower() for w in MALE_LEXICON]))\n",
    "\n",
    "print(\"Size of the female word bank:\", len(FEMALE_LEXICON))\n",
    "print(\"Size of the male word bank:\", len(MALE_LEXICON))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcfcfd",
   "metadata": {},
   "source": [
    "Having created the gendered lexicon we can now determine how many of the words are present in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_gendered_expressions(sentences: List[List[str]], male_wordlist: List[str], female_wordlist: List[str]):\n",
    "    # Helps make access O(1), in the future could have weights based on the gendered lexicon (e.g., how much skew)\n",
    "    male_exps, female_exps = {w: 1 for w in male_wordlist}, {w: 1 for w in female_wordlist}\n",
    "    \n",
    "    len_sents = []\n",
    "    male_counts = []\n",
    "    female_counts = []\n",
    "    for sent in sentences:\n",
    "        male_tks = [male_exps.get(t, 0) for t in sent]\n",
    "        female_tks = [female_exps.get(t, 0) for t in sent]\n",
    "        \n",
    "        len_sents.append(len(sent))\n",
    "        male_counts.append(sum(male_tks))\n",
    "        female_counts.append(sum(female_tks))\n",
    "    \n",
    "    return {\"male_counts\": male_counts, \"female_counts\": female_counts, \"counts\": len_sents}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# number of gendered expressions per sentence x benchmark\n",
    "# -----------------------------------------------------------\n",
    "results_gendered_lexicon = defaultdict(list)\n",
    "\n",
    "for dataset, templates in DATANAME_TO_TEMPLATES_CANONIC.items():\n",
    "    canonic_results = compute_num_gendered_expressions(templates, MALE_LEXICON, FEMALE_LEXICON)\n",
    "    \n",
    "    results_gendered_lexicon[\"dataset\"].extend([dataset] * len(templates))\n",
    "    # Number of words in MALE_LEXICON\n",
    "    results_gendered_lexicon[\"male_counts\"].extend(canonic_results[\"male_counts\"])\n",
    "    # Number of words in FEMALE_LEXICON\n",
    "    results_gendered_lexicon[\"female_counts\"].extend(canonic_results[\"female_counts\"])\n",
    "    # Number of words\n",
    "    results_gendered_lexicon[\"word_counts\"].extend(canonic_results[\"counts\"])\n",
    "    \n",
    "results_gendered_lexicon = pd.DataFrame(results_gendered_lexicon)\n",
    "results_gendered_lexicon.insert(3, \"male+female counts\", results_gendered_lexicon[\"male_counts\"] + results_gendered_lexicon[\"female_counts\"])\n",
    "results_gendered_lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average value per sentence in each dataset\") \n",
    "results_gendered_lexicon.groupby(\"dataset\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef933d",
   "metadata": {},
   "source": [
    "## Analysis 2 - Distribution of MaxGender across benchmarks\n",
    "\n",
    "We conjecture that accounting for gender correlations at a sentence level may lead to different evaluation results. in particular, we prompted ChatGPT to generate a benchmark with supposedly gender invariant sentences. \n",
    "\n",
    "In order to obtain a better idea of how \"gender-related\" our dataset is, we compute the distribution of MaxGender(s) per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to mapping from word to pmi diff for easy access\n",
    "WORD2PMI = PMI_DIFF[[\"word\", \"pmi_diff\"]].set_index(\"word\").to_dict()[\"pmi_diff\"]\n",
    "\n",
    "# Computes the pmi per each word in templates\n",
    "PMI_PER_TEMPLATES = {name: [] for name in DATANAMES}\n",
    "\n",
    "# Computes the pmi per word in each template\n",
    "PMIWORDS_PER_TEMPLATES = {name: [] for name in DATANAMES}\n",
    "\n",
    "\n",
    "for dataset, templates in DATANAME_TO_TEMPLATES_CANONIC.items():\n",
    "    for template in templates:\n",
    "        pmi = np.array([WORD2PMI.get(w) for w in template if WORD2PMI.get(w) is not None])\n",
    "        pmiwords = [(w, WORD2PMI.get(w)) for w in template if WORD2PMI.get(w) is not None]\n",
    "        \n",
    "        PMI_PER_TEMPLATES[dataset].append(pmi)\n",
    "        # one-to-one mapping between words and pmi\n",
    "        PMIWORDS_PER_TEMPLATES[dataset].append(pmiwords)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e350dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ORDERING = [\"Ours-05\", \"Ours-10\", \"Ours-20\", \"Winobias\", \"Winogender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_gender_max(sentence_pmis: List[float]) -> float:\n",
    "    \"\"\"Determines the maximum absolute gender correlation in the provided list.\"\"\"\n",
    "    if len(sentence_pmis) > 0:\n",
    "        idx = np.argmax(np.abs(sentence_pmis))\n",
    "        return sentence_pmis[idx]\n",
    "    \n",
    "def compute_measure_per_sentence(\n",
    "    pmi_per_templates: Dict[str, List[List[float]]],\n",
    "    measure_fn: callable,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Applies the measure function to the PMI-scores associated with each template.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pmi_per_templates: Dict[str, List[List[float]]]\n",
    "        The mapping between the datasets and the list of templates scores.\n",
    "        Each template score is a list of scores (potentially one per each word).\n",
    "        \n",
    "    measure_fn: callable(List[float]) -> float\n",
    "        Aggregating function of the list of scores assigned to each template.\n",
    "        One example could be 'sentence_gender_max'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A long table containing a score per template in each dataset (dubbed 'value'),\n",
    "        as well as whether it is invalid (i.e., empty list of scores).\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    for dataset, sentences in pmi_per_templates.items():\n",
    "        for ix, sentence in enumerate(sentences):\n",
    "            val = measure_fn(sentence)\n",
    "\n",
    "            results[\"dataset\"].append(dataset)\n",
    "            # for this work, the sentence must not have been sorted before in this notebook!!\n",
    "            results[\"template_idx\"].append(ix)\n",
    "            results[\"value\"].append(val)\n",
    "            results[\"is_invalid\"].append(len(sentence) == 0)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97745f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Compute the gender max metric per sentence\n",
    "# -------------------------------------------------------\n",
    "RESULTS_GENDER_MAX_PER_SENT = compute_measure_per_sentence(PMI_PER_TEMPLATES, sentence_gender_max)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# plot the gender max per sentence\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(6, 4), dpi=150)\n",
    "sns.boxplot(RESULTS_GENDER_MAX_PER_SENT, x=\"dataset\", y=\"value\")\n",
    "plt.ylim(-3, 3)\n",
    "plt.ylabel(\"Gender Max word-PMI (per sentence)\")\n",
    "plt.show()\n",
    "\n",
    "# Table results\n",
    "RESULTS_GENDER_MAX_PER_SENT[[\"dataset\", \"value\"]].groupby(\"dataset\").describe().T[DATASET_ORDERING].style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd9bd7",
   "metadata": {},
   "source": [
    "#### Number of templates per dataset (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fade5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of templates per dataset\n",
    "num_templates = RESULTS_GENDER_MAX_PER_SENT.groupby(\"dataset\").count()[[\"value\"]]\n",
    "num_templates.rename({\"value\": \"orig_num_templates\"}, axis=1, inplace=True)\n",
    "num_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134c36d",
   "metadata": {},
   "source": [
    "## Analysis 3: Impact of $\\varepsilon_k$  on benchmark size?\n",
    "\n",
    "In this section, we assess the impact of different constraints on the strength of the words' gender correlations in the benchmark size. To do this, we first define a set of `CONSTRAINT_EPSILONS` linearly spaced between 0.5 and 1.5, and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_col_val(\n",
    "    data: pd.DataFrame, \n",
    "    col: str=\"value\",\n",
    "    thres: float=1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns a slice of `data` whose `col`'s value is within `[-thres, thres]`.\"\"\"\n",
    "    return data[(data[col] >= -thres) & (data[col] <= thres)].copy()\n",
    "\n",
    "CONSTRAINT_EPSILONS = np.linspace(0.2, 1.5, 51)[::-1]\n",
    "\n",
    "filter_templates_results = {3: RESULTS_GENDER_MAX_PER_SENT.groupby(\"dataset\").count()[\"value\"]}\n",
    "for eps in CONSTRAINT_EPSILONS:\n",
    "    # number of examples after removing outliers outside [-1, 1]    \n",
    "    df_eps = filter_data_by_col_val(RESULTS_GENDER_MAX_PER_SENT, thres=eps)\n",
    "    \n",
    "    # Obtain the number of remaining templates\n",
    "    templ_diff = df_eps.groupby(\"dataset\").count()[\"value\"]\n",
    "    # Obtain the difference in template counts by applying a specific filter.\n",
    "    #templ_diff = df_eps.groupby(\"dataset\").count()[\"value\"] - num_templates[\"orig_num_templates\"]\n",
    "    filter_templates_results[round(eps, 2)] = templ_diff\n",
    "    \n",
    "# How many templates we loose as we increase the filter\n",
    "filter_templates_results =  pd.DataFrame(filter_templates_results).T\n",
    "filter_templates_results = filter_templates_results.reset_index().rename({\"index\": \"filter\"}, axis=1)\n",
    "filter_templates_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "for dataset in DATANAMES:\n",
    "    sns.lineplot(filter_templates_results, x=\"filter\", y=dataset, label=dataset, lw=1)\n",
    "\n",
    "plt.xlabel(\"Max word-level gender correlation allowed\")\n",
    "plt.ylabel(\"Number of templates\")\n",
    "plt.legend( loc='upper left', bbox_to_anchor=(1, 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_templates_results = filter_templates_results.copy() \n",
    "_filter_templates_results[DATASET_ORDERING] = (_filter_templates_results[DATASET_ORDERING] / _filter_templates_results[DATASET_ORDERING].iloc[0]).round(2) \n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "for dataset in DATANAMES:\n",
    "    sns.lineplot(_filter_templates_results, x=\"filter\", y=dataset, label=dataset, lw=1)\n",
    "\n",
    "plt.xlabel(\"Maximum word-level correlation ($\\epsilon$)\")\n",
    "plt.ylabel(\"Benchmark size fraction\")\n",
    "#plt.legend( loc='upper left', bbox_to_anchor=(0, -0.2), ncols=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93631bbe",
   "metadata": {},
   "source": [
    "### Distribution of $\\mathrm{MaxPMI(s)}$ across benchmarks\n",
    "\n",
    "In this section, we determine what is the average distribution of $\\mathrm{MaxPMI(s)}$ per sentence. Any threshold $\\varepsilon_k$ will cap the distribution in a symmetric fashion around $0$. That is, we enforce constraints of the type: $|\\mathrm{MaxPMI(s)}| \\leq \\varepsilon_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILTER_BY_1 = filter_data_by_col_val(RESULTS_GENDER_MAX_PER_SENT, thres=0.25)\n",
    "sns.boxplot(RESULTS_FILTER_BY_1, x=\"dataset\", y=\"value\")\n",
    "plt.ylim(-3, 3)\n",
    "plt.ylabel(\"Gender Max word-PMI (per sentence)\")\n",
    "plt.show()\n",
    "\n",
    "RESULTS_FILTER_BY_1[[\"dataset\", \"value\"]].groupby(\"dataset\").describe().T[DATASET_ORDERING].style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36202b",
   "metadata": {},
   "source": [
    "## Check outlier examples\n",
    "\n",
    "In this section, you can examine which examples are filtered out using $\\epsilon_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35669326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(original_file: pd.DataFrame, dataset: str, gender_max: pd.DataFrame, col=\"value\", eps=1.0):\n",
    "    original_file = original_file.copy()\n",
    "    \n",
    "    # make sure the gender_max is specific to our original dataset (shouldn't have any ordering other than the index)\n",
    "    max_df = gender_max[gender_max[\"dataset\"] == dataset]\n",
    "    assert (np.array(original_file.index) == max_df[\"template_idx\"].values).all(), \"Index mismatch\"\n",
    "    # keep templates whose col value is within [-eps, eps]\n",
    "    filtered_df = filter_data_by_col_val(max_df, col=col, thres=eps)\n",
    "    \n",
    "    # add information about the value and whether we were able to obtain any PMI value for that sentence\n",
    "    for c in (col, \"is_invalid\"):\n",
    "        original_file[c] = max_df[c].values\n",
    "    #print(max_df[\"value\"].head())\n",
    "    #print(\"=====\")\n",
    "    #print(original_file[\"value\"].head())\n",
    "    # Mark every example to be dropped by default\n",
    "    original_file[\"is_dropped\"] = True\n",
    "    # Collect a mask w/ the indication of whether that template is to be kept or not\n",
    "    keep_mask = original_file.index.isin(filtered_df[\"template_idx\"])\n",
    "    original_file.loc[keep_mask, \"is_dropped\"] = False    \n",
    "    return original_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51741ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1 = {\n",
    "    k: filter_dataframe(f, k, RESULTS_GENDER_MAX_PER_SENT, eps=1)\n",
    "    for k, f in DATASET_2_FILES.items()\n",
    "}\n",
    "\n",
    "for name, df in DROPPED_EXAMPLES_EPS1.items():\n",
    "    print(\"---\", name, \"---\\n\", df[\"is_dropped\"].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1[\"Ours-05\"][DROPPED_EXAMPLES_EPS1[\"Ours-05\"][\"is_dropped\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c537dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1[\"Ours-10\"][DROPPED_EXAMPLES_EPS1[\"Ours-10\"][\"is_dropped\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae7c11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1[\"Ours-20\"][DROPPED_EXAMPLES_EPS1[\"Ours-20\"][\"is_dropped\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c709a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1[\"Winobias\"][DROPPED_EXAMPLES_EPS1[\"Winobias\"][\"is_dropped\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cbdcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DROPPED_EXAMPLES_EPS1[\"Winogender\"][DROPPED_EXAMPLES_EPS1[\"Winogender\"][\"is_dropped\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_d = DROPPED_EXAMPLES_EPS1[\"Ours-10\"]\n",
    "_d[_d[\"sentence\"].apply(lambda x: \"captivating\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
