{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CAMERA-READY PLOTTING (thanks Alex Boyd!)\n",
    "# -----------------------------------------------------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "# The following code is borrowed from material provided by Alex!\n",
    "FULL_WIDTH = 5.50107\n",
    "COL_WIDTH  = 4.50461\n",
    "\n",
    "# Accessibility\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "matplotlib.rcParams[\"axes.prop_cycle\"] = matplotlib.cycler(color=sns.color_palette(\"colorblind\"))\n",
    "\n",
    "# Put at top of plotting script (requires tex be installed though)\n",
    "matplotlib.rc('font', family='serif', size=20)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def adjust(fig, left=0.0, right=1.0, bottom=0.0, top=1.0, wspace=0.0, hspace=0.0):\n",
    "    fig.subplots_adjust(\n",
    "        left   = left,  # the left side of the subplots of the figure\n",
    "        right  = right,  # the right side of the subplots of the figure\n",
    "        bottom = bottom,  # the bottom of the subplots of the figure\n",
    "        top    = top,  # the top of the subplots of the figure\n",
    "        wspace = wspace,  # the amount of width reserved for blank space between subplots\n",
    "        hspace = hspace,  # the amount of height reserved for white space between subplots\n",
    "    )\n",
    "    \n",
    "def save_fig(fig, name, **kwargs):\n",
    "    fig.savefig(f\"./camera_ready/images/{name}.pdf\", bbox_inches=\"tight\", **kwargs)\n",
    "\n",
    "def disable_axis(ax):\n",
    "    ax.set_zorder(-100)  # Avoids a visual rendering bug\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.setp(ax.spines.values(), color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Load model files\n",
    "\n",
    "Run `post-process-results.ipynb` first to generate a compiled version of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0969d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset names:\n",
      "  -> ['USE-5', 'Winobias', 'Winogender', 'USE-10', 'USE-20'] \n",
      "\n",
      "Number of evaluated models for dataset USE-5 is 28\n",
      "Number of evaluated models for dataset Winobias is 28\n",
      "Number of evaluated models for dataset Winogender is 28\n",
      "Number of evaluated models for dataset USE-10 is 28\n",
      "Number of evaluated models for dataset USE-20 is 28\n",
      "Evaluating 28 models:\n",
      " - Mistral-7B-v0.1\n",
      " - Mixtral-8x7B-v0.1\n",
      " - OLMo-1B\n",
      " - OLMo-7B\n",
      " - gpt-j-6b\n",
      " - llama-2-13b\n",
      " - llama-2-70b\n",
      " - llama-2-7b\n",
      " - mpt-30b\n",
      " - mpt-7b\n",
      " - opt-125m\n",
      " - opt-2.7b\n",
      " - opt-350m\n",
      " - opt-6.7b\n",
      " - pythia-1.4b\n",
      " - pythia-1.4b (D)\n",
      " - pythia-12b\n",
      " - pythia-12b (D)\n",
      " - pythia-160m\n",
      " - pythia-160m (D)\n",
      " - pythia-2.8b\n",
      " - pythia-2.8b (D)\n",
      " - pythia-410m\n",
      " - pythia-410m (D)\n",
      " - pythia-6.9b\n",
      " - pythia-6.9b (D)\n",
      " - pythia-70m\n",
      " - pythia-70m (D)\n"
     ]
    }
   ],
   "source": [
    "RESULTS_DIR = \"../results\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"USE-5\": f\"{RESULTS_DIR}/USE-5-no-maxpmi-constraint.csv.gz\",\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": f\"{RESULTS_DIR}/Winobias-no-maxpmi-constraint.csv.gz\",\n",
    "    \"Winogender\": f\"{RESULTS_DIR}/Winogender-no-maxpmi-constraint.csv.gz\",\n",
    "    # We define this ordering so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"USE-10\": f\"{RESULTS_DIR}/USE-10-no-maxpmi-constraint.csv.gz\",\n",
    "    \"USE-20\": f\"{RESULTS_DIR}/USE-20-no-maxpmi-constraint.csv.gz\",\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES, \"\\n\")\n",
    "\n",
    "# Read each individual filepath, creating an association <str, dataframe>.\n",
    "# every str should have a list of the same size.\n",
    "DATASET_2_FILES = {name: pd.read_csv(fp) for name, fp in DATASET_2_FILEPATHS.items()}\n",
    "DATASET_2_FILES = {name: df.sort_values([\"model\", \"orig_index\"]).reset_index(drop=True) for name, df in DATASET_2_FILES.items()}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Determine whether the number of evaluated models are the same\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "MODELS, NUM_EVAL_MODELS = [], []\n",
    "\n",
    "for dataset, df in DATASET_2_FILES.items():\n",
    "    print(\"Number of evaluated models for dataset\", dataset, \"is\", df[\"model\"].nunique())\n",
    "    MODELS.extend(df[\"model\"].unique())\n",
    "    NUM_EVAL_MODELS.append(df[\"model\"].nunique())\n",
    "    \n",
    "# We force the number of models to be the same across all datasets\n",
    "if len(set(NUM_EVAL_MODELS)) != 1:\n",
    "    warnings.warn(f\"Inconsistent number of models across the different evaluation mber models: {NUM_EVAL_MODELS}\")\n",
    "\n",
    "NUM_EVAL_MODELS = NUM_EVAL_MODELS[0]\n",
    "print(\"Evaluating\", NUM_EVAL_MODELS, \"models:\")\n",
    "MODELS = list(sorted(set(MODELS)))\n",
    "print(\" -\", \"\\n - \".join(MODELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22e3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking slices for dataset: USE-5\n",
      "Checking slices for dataset: Winobias\n",
      "Checking slices for dataset: Winogender\n",
      "Checking slices for dataset: USE-10\n",
      "Checking slices for dataset: USE-20\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Validation (!sanity check)\n",
    "# ------------------------------------------------------------------------\n",
    "# When selecting a data slice from the big dataframe\n",
    "# we must guarantee that the sentences match to one another\n",
    "# (that is necessary because the remaining of the code is relying\n",
    "# on ordering of the dataframes)\n",
    "def check_slices(dataset: pd.DataFrame, data2files: dict, models: List[str]):\n",
    "    \"\"\"Check for the ordering of the rows in ``dataset`` correspond to the\n",
    "    ones in ``data2files``. Since the data2files are ordered by models,\n",
    "    we will focus on that.\"\"\"\n",
    "    slices = []\n",
    "    for model in models:\n",
    "        df = data2files[dataset]\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "        if len(slices) > 1:\n",
    "            assert np.array_equal(slices[-1][\"template\"].values, df[\"template\"].values)    \n",
    "        slices.append(df)\n",
    "        \n",
    "    \n",
    "for dataset in DATASET_NAMES:\n",
    "    print(\"Checking slices for dataset:\", dataset)\n",
    "    check_slices(dataset=dataset, data2files=DATASET_2_FILES, models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045a3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column max_gender_pmi for values [2.5   2.475 2.45  2.425 2.4   2.375 2.35  2.325 2.3   2.275 2.25  2.225\n",
      " 2.2   2.175 2.15  2.125 2.1   2.075 2.05  2.025 2.    1.975 1.95  1.925\n",
      " 1.9   1.875 1.85  1.825 1.8   1.775 1.75  1.725 1.7   1.675 1.65  1.625\n",
      " 1.6   1.575 1.55  1.525 1.5   1.475 1.45  1.425 1.4   1.375 1.35  1.325\n",
      " 1.3   1.275 1.25  1.225 1.2   1.175 1.15  1.125 1.1   1.075 1.05  1.025\n",
      " 1.    0.975 0.95  0.925 0.9   0.875 0.85  0.825 0.8   0.775 0.75  0.725\n",
      " 0.7   0.675 0.65  0.625 0.6   0.575 0.55  0.525 0.5   0.475 0.45  0.425\n",
      " 0.4   0.375 0.35  0.325 0.3   0.275 0.25  0.225 0.2   0.175 0.15  0.125\n",
      " 0.1   0.075 0.05  0.025 0.   ]\n"
     ]
    }
   ],
   "source": [
    "from metrics import filter_eta_and_count_examples\n",
    "\n",
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "FILTERING_ETA = np.linspace(0.0, 2.5, 101)[::-1]\n",
    "print(\"Processing column\", MAXGENDER_COL, \"for values\", FILTERING_ETA)\n",
    "\n",
    "FILTER_CURVES_RESULTS = filter_eta_and_count_examples(\n",
    "    name_and_dataset=DATASET_2_FILES,\n",
    "    etas=FILTERING_ETA,\n",
    "    col=MAXGENDER_COL,\n",
    "    constant=NUM_EVAL_MODELS, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe331f",
   "metadata": {},
   "source": [
    "## Fairness metrics - Fixed threshold & AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "# fairness col in natural log space\n",
    "FAIRNESS_COL = \"FM_logprob\"\n",
    "\n",
    "# probability space threshold\n",
    "_FAIRNESS_THRESHOLD = 1.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293f8053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21748394421390624\n"
     ]
    }
   ],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f271c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Using threshold: 0.2175 to compute fairness metric\n",
      "--------------------------------------------------------------------------------\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*80)\n",
    "print(f\"Using threshold: {FAIRNESS_THRESHOLD:.4f} to compute fairness metric\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Original dataset (before applying any of the max pmi constraints)\n",
    "BEFORE_FILTER = {dataset: df.copy() for dataset, df in DATASET_2_FILES.items()}\n",
    "\n",
    "# Use this version to use the natural logarithm\n",
    "# BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, 0.5)\n",
    "# use this version to use the base 10 results\n",
    "BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5375ec",
   "metadata": {},
   "source": [
    "### Neutrality and AuFC (per constrained setting)\n",
    "\n",
    "While we propose a pipeline to create benchmarks that satisfy the gender co-occurrence constraints, in our experiments we do not immediately restrict our benchmarks. The main goal being that we'd like to be able to study the effect of stricter PMI constraints. For that reason, in the following setting, we will compute the value of Neutrality and AuFC for $\\eta \\in \\{0.3, 0.5, 0.65, 0.8, 1\\}$. The stricter setup being $\\eta = 0.3$ and the least strict being $\\eta = 1$. The original unconstrained version of the dataset (stored in variable `BEFORE_FILTER[<dataset>]`) is denoted $\\eta = \\infty$ in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1053999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness col: 'FM_logprob' and threshold: '0.21748394421390624'\n",
      "eta = 0.3\n",
      "eta = 0.5\n",
      "eta = 0.65\n",
      "eta = 0.8\n",
      "eta = 1.0\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n",
      "FM_logprob_base10 0.21748394421390624\n"
     ]
    }
   ],
   "source": [
    "PMI_THRESHOLDS = [0.3, 0.5, 0.65, 0.8, 1.0]\n",
    "\n",
    "print(f\"Fairness col: '{FAIRNESS_COL}' and threshold: '{FAIRNESS_THRESHOLD}'\")\n",
    "AFTER_FILTER = {}\n",
    "# Filter out the dataset_w_constraints according to the different PMI thresholds (or \\epsilon_k)\n",
    "for pmi_threshold in PMI_THRESHOLDS:\n",
    "    # Create the different filters for each dataset\n",
    "    print(\"eta =\", pmi_threshold)\n",
    "    AFTER_FILTER[pmi_threshold] = {\n",
    "        dataset: filter_data_by_col_val(df.copy(), col=MAXGENDER_COL, thres=pmi_threshold).copy()\n",
    "        for dataset, df in BEFORE_FILTER.items()\n",
    "    } \n",
    "\n",
    "# For each filtered version of the dataset, compute the corresponding skews and metrics\n",
    "AFTER_FILTER = {\n",
    "    filt: compute_skews_(bias_files, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base) for filt, bias_files in AFTER_FILTER.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eddf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results(data2files) -> pd.DataFrame:\n",
    "    return pd.merge(\n",
    "        # Compute unstereo score\n",
    "        compute_neutral_pct_w_std(data2files), \n",
    "        # Compute predictive disparity metric\n",
    "        compute_female_male_skews(data2files, MODELS),\n",
    "        on=[\"dataset\", \"model\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "METRICS_BEFORE_FILTER = merge_results(BEFORE_FILTER)\n",
    "METRICS_AFTER_FILTER = {eta: merge_results(AFTER_FILTER[eta]) for eta in AFTER_FILTER.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dcc6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All examples:\n",
      "{'USE-5': 4405.0, 'Winobias': 1586.0, 'Winogender': 240.0, 'USE-10': 4740.0, 'USE-20': 4839.0}\n",
      "\n",
      "Number of examples after filter 0.3\n",
      "{'USE-5': 1556.0, 'Winobias': 22.0, 'Winogender': 16.0, 'USE-10': 601.0, 'USE-20': 133.0}\n",
      "\n",
      "Number of examples after filter 0.5\n",
      "{'USE-5': 3069.0, 'Winobias': 186.0, 'Winogender': 69.0, 'USE-10': 2397.0, 'USE-20': 1456.0}\n",
      "\n",
      "Number of examples after filter 0.65\n",
      "{'USE-5': 3698.0, 'Winobias': 409.0, 'Winogender': 107.0, 'USE-10': 3401.0, 'USE-20': 2828.0}\n",
      "\n",
      "Number of examples after filter 0.8\n",
      "{'USE-5': 3978.0, 'Winobias': 675.0, 'Winogender': 150.0, 'USE-10': 3916.0, 'USE-20': 3561.0}\n",
      "\n",
      "Number of examples after filter 1.0\n",
      "{'USE-5': 4263.0, 'Winobias': 879.0, 'Winogender': 188.0, 'USE-10': 4396.0, 'USE-20': 4296.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"All examples:\")\n",
    "print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in BEFORE_FILTER.items()})\n",
    "\n",
    "\n",
    "for eps, eps_values in AFTER_FILTER.items():\n",
    "    print()\n",
    "    print(\"Number of examples after filter\", eps)\n",
    "    print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in eps_values.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149977",
   "metadata": {},
   "source": [
    "### Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b624cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def model2latex(model: str):    \n",
    "    if \"pythia\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"pythia-(.+)\", r\"pyths{\\1}\", model)\n",
    "    elif \"opt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"opt-(.+)\", r\"opts{\\1}\", model)\n",
    "    elif \"mpt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"mpt-(.+)\", r\"mpts{\\1}\", model)\n",
    "    elif \"llama-2\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"llama-2-(.+)\", r\"llamas{\\1}\", model)\n",
    "    elif \"gpt-j\" in model:\n",
    "        return \"\\\\\" + \"gptj\"\n",
    "    else:\n",
    "        return model\n",
    "        \n",
    "\n",
    "def print_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    print(table.set_index(\"model\").to_latex())\n",
    "\n",
    "    \n",
    "def get_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    return table.set_index(\"model\")\n",
    "\n",
    "\n",
    "def print_results_aufc(data_auc, filepath):\n",
    "    table = pd.pivot(data_auc, values=[\"auc\"], index=\"model\", columns=[\"dataset_\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(table_str)\n",
    "    \n",
    "    # To latex file, leveraging rendering commands for model names\n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    print(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31021f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a file for landing page with the different metrics.\n",
    "# Ideally, we create a different json file for every dataset\n",
    "# where json file contains for every filter/max pmi constraint the models'\n",
    "# values for a given metric.\n",
    "# -----------------------\n",
    "# Example for dataset X\n",
    "# -----------------------\n",
    "# {\n",
    "#   none: {\n",
    "#     neutral__avg: {\n",
    "#        model1: 98.32,\n",
    "#        ...\n",
    "#        modeln: ...\n",
    "#     }, \n",
    "#     neutral__std: {\n",
    "#\n",
    "#     },   \n",
    "#     aufc: {\n",
    "#\n",
    "#     },  \n",
    "#     male_rel_ratio: {\n",
    "#\n",
    "#     },      \n",
    "#   },\n",
    "#   0.5: {\n",
    "#     ...\n",
    "#   },\n",
    "#   ...\n",
    "# }\n",
    "# ---------------------------------------------------------------\n",
    "METRICS_FOR_LANDING_PAGE = {name: {} for name in DATASET_NAMES}\n",
    "\n",
    "neutral__avg = {None: compute_female_male_skews(BEFORE_FILTER, MODELS)}\n",
    "neutral__std = {None: compute_neutral_pct_w_std(BEFORE_FILTER)}\n",
    "\n",
    "for eps in AFTER_FILTER.keys():\n",
    "    neutral__avg[eps] = compute_female_male_skews(AFTER_FILTER[eps], MODELS)\n",
    "    neutral__std[eps] = compute_neutral_pct_w_std(AFTER_FILTER[eps])\n",
    "\n",
    "# None is the unconstrained / original dataset before any MAXPMI Constraint were applied \n",
    "fair_auc_landing_page = {None: compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")[1]}\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    _, fair_auc = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    fair_auc_landing_page[eps] = fair_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f4815a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21748394421390624\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "FILTER = 0.3\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "FILTER = 0.5\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "FILTER = 0.65\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "FILTER = 0.8\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "FILTER = 1.0\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "# AUFC_BASE_DIR = \"./camera_ready/table/aufc\"\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")\n",
    "\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "# print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/unfiltered.tex\")\n",
    "\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "    # print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/filter_{str(eps).replace('.', '')}.tex\")\n",
    "    fair_auc_landing_page[eps] = FAIR_AUC\n",
    "    # Uncomment these lines for drawing fairness plots\n",
    "    # fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models)\n",
    "    # fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, misc_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1c7e05",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/processed-results/USE-5.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m     METRICS_FOR_LANDING_PAGE[dataset] \u001b[38;5;241m=\u001b[39m eps_results\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/processed-results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(eps_results, f, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/processed-results/all_datasets.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/projects/tokenization-proj/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/processed-results/USE-5.json'"
     ]
    }
   ],
   "source": [
    "for dataset in DATASET_NAMES:\n",
    "    # fairness_neutral__avg\n",
    "    eps_results = {}\n",
    "    for eps in [None] + PMI_THRESHOLDS:\n",
    "        results = {}\n",
    "        neutral_subset = neutral__std[eps]\n",
    "        # print(dataset, eps,neutral[\"dataset\"].unique())\n",
    "        neutral_subset = neutral_subset[neutral_subset[\"dataset\"] == (\"USE-05\" if dataset == \"USE-5\" else dataset)].drop(\"neutral_final\", axis=1)\n",
    "\n",
    "        results[\"neutral__avg\"] = {}\n",
    "        results[\"neutral__std\"] = {}\n",
    "        for i, row in neutral_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"neutral__avg\"][model] = row[\"neutral_avg\"]\n",
    "            results[\"neutral__std\"][model] = row[\"neutral_std\"]\n",
    "    \n",
    "        ## AUFC \n",
    "        results[\"aufc\"] = {}\n",
    "        fair_auc = fair_auc_landing_page[eps]\n",
    "        aufc_subset = fair_auc[fair_auc.dataset == dataset]\n",
    "        for i, row in aufc_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"aufc\"][model] = row[\"auc\"]\n",
    "        \n",
    "        ## Male Relative ratio\n",
    "        male_fem_subset = neutral__avg[eps]\n",
    "        male_fem_subset = male_fem_subset[male_fem_subset.dataset == (\"USE-05\" if dataset == \"USE-5\" else dataset)]\n",
    "        results[\"male_rel_ratio\"] = {}\n",
    "        results[\"num_examples_nonneutral\"] = {}\n",
    "        results[\"num_examples\"] = male_fem_subset[\"total\"].unique().item()\n",
    "        for i, row in male_fem_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"male_rel_ratio\"][model] = row[\"partial_pct_mal\"]\n",
    "            results[\"num_examples_nonneutral\"][model] = row[\"counts_fem\"] + row[\"counts_mal\"]\n",
    "        if eps == None:\n",
    "            eps = \"unconstrained\"\n",
    "        eps_results[eps] = results\n",
    "        \n",
    "    METRICS_FOR_LANDING_PAGE[dataset] = eps_results\n",
    "    import json\n",
    "    with open(f\"../results/processed-results/{dataset}.json\", \"w\") as f:\n",
    "        json.dump(eps_results, f, sort_keys=False, indent=2)\n",
    "\n",
    "\n",
    "with open(f\"../results/processed-results/all_datasets.json\", \"w\") as f:\n",
    "    json.dump(METRICS_FOR_LANDING_PAGE, f, sort_keys=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776462-0b1f-4055-b08a-38c022a93968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
