{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Data Loading: Load PMI difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1feeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GROUP_PAIRED_WORDLIST, FEMALE_WORDS, MALE_WORDS, get_pmi_diff, get_gender_pairs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792ee5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152515\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmi__her</th>\n",
       "      <th>pmi__his</th>\n",
       "      <th>pmi__him</th>\n",
       "      <th>pmi__hers</th>\n",
       "      <th>pmi__mother</th>\n",
       "      <th>pmi__father</th>\n",
       "      <th>pmi__mom</th>\n",
       "      <th>pmi__dad</th>\n",
       "      <th>pmi__mummy</th>\n",
       "      <th>pmi__daddy</th>\n",
       "      <th>...</th>\n",
       "      <th>pmi__queen</th>\n",
       "      <th>pmi__king</th>\n",
       "      <th>pmi__queens</th>\n",
       "      <th>pmi__kings</th>\n",
       "      <th>pmi__princess</th>\n",
       "      <th>pmi__prince</th>\n",
       "      <th>pmi__princesses</th>\n",
       "      <th>pmi__princes</th>\n",
       "      <th>pmi__he</th>\n",
       "      <th>pmi__she</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>80439.000000</td>\n",
       "      <td>98771.000000</td>\n",
       "      <td>65608.000000</td>\n",
       "      <td>7537.000000</td>\n",
       "      <td>30706.000000</td>\n",
       "      <td>29684.000000</td>\n",
       "      <td>10998.000000</td>\n",
       "      <td>10495.000000</td>\n",
       "      <td>1717.000000</td>\n",
       "      <td>2977.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10119.000000</td>\n",
       "      <td>19446.000000</td>\n",
       "      <td>3313.000000</td>\n",
       "      <td>6617.000000</td>\n",
       "      <td>5412.000000</td>\n",
       "      <td>8203.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>3825.000000</td>\n",
       "      <td>100828.000000</td>\n",
       "      <td>66891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-24.827642</td>\n",
       "      <td>-24.861843</td>\n",
       "      <td>-24.803681</td>\n",
       "      <td>-24.205257</td>\n",
       "      <td>-24.915487</td>\n",
       "      <td>-24.912725</td>\n",
       "      <td>-25.195694</td>\n",
       "      <td>-25.311220</td>\n",
       "      <td>-24.439117</td>\n",
       "      <td>-25.298108</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.361834</td>\n",
       "      <td>-25.395301</td>\n",
       "      <td>-24.835216</td>\n",
       "      <td>-24.729553</td>\n",
       "      <td>-25.019586</td>\n",
       "      <td>-25.328138</td>\n",
       "      <td>-23.698000</td>\n",
       "      <td>-23.932025</td>\n",
       "      <td>-25.416424</td>\n",
       "      <td>-25.262707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.563299</td>\n",
       "      <td>1.580690</td>\n",
       "      <td>1.506580</td>\n",
       "      <td>1.445382</td>\n",
       "      <td>1.324532</td>\n",
       "      <td>1.342077</td>\n",
       "      <td>1.403486</td>\n",
       "      <td>1.310585</td>\n",
       "      <td>1.538892</td>\n",
       "      <td>1.450787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435780</td>\n",
       "      <td>1.500407</td>\n",
       "      <td>1.723579</td>\n",
       "      <td>1.696653</td>\n",
       "      <td>1.477100</td>\n",
       "      <td>1.462137</td>\n",
       "      <td>1.718849</td>\n",
       "      <td>1.715749</td>\n",
       "      <td>1.514534</td>\n",
       "      <td>1.499545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-33.499275</td>\n",
       "      <td>-33.331670</td>\n",
       "      <td>-33.613325</td>\n",
       "      <td>-30.640672</td>\n",
       "      <td>-30.936595</td>\n",
       "      <td>-30.878193</td>\n",
       "      <td>-30.376505</td>\n",
       "      <td>-30.894450</td>\n",
       "      <td>-29.312282</td>\n",
       "      <td>-30.237593</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.485745</td>\n",
       "      <td>-31.237119</td>\n",
       "      <td>-28.636829</td>\n",
       "      <td>-29.793686</td>\n",
       "      <td>-30.302262</td>\n",
       "      <td>-31.337065</td>\n",
       "      <td>-30.067307</td>\n",
       "      <td>-29.995963</td>\n",
       "      <td>-32.885207</td>\n",
       "      <td>-33.474327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-25.685422</td>\n",
       "      <td>-25.682861</td>\n",
       "      <td>-25.638325</td>\n",
       "      <td>-25.184315</td>\n",
       "      <td>-25.755291</td>\n",
       "      <td>-25.742207</td>\n",
       "      <td>-26.175983</td>\n",
       "      <td>-26.204275</td>\n",
       "      <td>-25.419204</td>\n",
       "      <td>-26.245775</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.331394</td>\n",
       "      <td>-26.407824</td>\n",
       "      <td>-25.942235</td>\n",
       "      <td>-25.915272</td>\n",
       "      <td>-26.010199</td>\n",
       "      <td>-26.314027</td>\n",
       "      <td>-24.823475</td>\n",
       "      <td>-25.134534</td>\n",
       "      <td>-26.219694</td>\n",
       "      <td>-26.079852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-24.622905</td>\n",
       "      <td>-24.552552</td>\n",
       "      <td>-24.605622</td>\n",
       "      <td>-24.219955</td>\n",
       "      <td>-24.857188</td>\n",
       "      <td>-24.823301</td>\n",
       "      <td>-25.178166</td>\n",
       "      <td>-25.242244</td>\n",
       "      <td>-24.648881</td>\n",
       "      <td>-25.315466</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.429962</td>\n",
       "      <td>-25.440690</td>\n",
       "      <td>-25.287083</td>\n",
       "      <td>-24.966771</td>\n",
       "      <td>-25.088823</td>\n",
       "      <td>-25.347642</td>\n",
       "      <td>-24.085664</td>\n",
       "      <td>-24.178638</td>\n",
       "      <td>-25.137871</td>\n",
       "      <td>-25.078310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-23.743398</td>\n",
       "      <td>-23.767377</td>\n",
       "      <td>-23.746094</td>\n",
       "      <td>-23.240134</td>\n",
       "      <td>-24.040494</td>\n",
       "      <td>-24.000929</td>\n",
       "      <td>-24.242664</td>\n",
       "      <td>-24.399705</td>\n",
       "      <td>-23.807955</td>\n",
       "      <td>-24.462129</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.524708</td>\n",
       "      <td>-24.469532</td>\n",
       "      <td>-24.229567</td>\n",
       "      <td>-23.792893</td>\n",
       "      <td>-24.122755</td>\n",
       "      <td>-24.380551</td>\n",
       "      <td>-22.893234</td>\n",
       "      <td>-22.914827</td>\n",
       "      <td>-24.351869</td>\n",
       "      <td>-24.212292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-20.458226</td>\n",
       "      <td>-20.727246</td>\n",
       "      <td>-19.520618</td>\n",
       "      <td>-18.544282</td>\n",
       "      <td>-18.927156</td>\n",
       "      <td>-19.795987</td>\n",
       "      <td>-17.653614</td>\n",
       "      <td>-18.349328</td>\n",
       "      <td>-16.823430</td>\n",
       "      <td>-15.609870</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.835727</td>\n",
       "      <td>-18.720616</td>\n",
       "      <td>-16.770424</td>\n",
       "      <td>-17.806201</td>\n",
       "      <td>-17.986234</td>\n",
       "      <td>-18.367298</td>\n",
       "      <td>-15.972031</td>\n",
       "      <td>-16.816626</td>\n",
       "      <td>-20.899774</td>\n",
       "      <td>-21.228257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pmi__her      pmi__his      pmi__him    pmi__hers   pmi__mother  \\\n",
       "count  80439.000000  98771.000000  65608.000000  7537.000000  30706.000000   \n",
       "mean     -24.827642    -24.861843    -24.803681   -24.205257    -24.915487   \n",
       "std        1.563299      1.580690      1.506580     1.445382      1.324532   \n",
       "min      -33.499275    -33.331670    -33.613325   -30.640672    -30.936595   \n",
       "25%      -25.685422    -25.682861    -25.638325   -25.184315    -25.755291   \n",
       "50%      -24.622905    -24.552552    -24.605622   -24.219955    -24.857188   \n",
       "75%      -23.743398    -23.767377    -23.746094   -23.240134    -24.040494   \n",
       "max      -20.458226    -20.727246    -19.520618   -18.544282    -18.927156   \n",
       "\n",
       "        pmi__father      pmi__mom      pmi__dad   pmi__mummy   pmi__daddy  \\\n",
       "count  29684.000000  10998.000000  10495.000000  1717.000000  2977.000000   \n",
       "mean     -24.912725    -25.195694    -25.311220   -24.439117   -25.298108   \n",
       "std        1.342077      1.403486      1.310585     1.538892     1.450787   \n",
       "min      -30.878193    -30.376505    -30.894450   -29.312282   -30.237593   \n",
       "25%      -25.742207    -26.175983    -26.204275   -25.419204   -26.245775   \n",
       "50%      -24.823301    -25.178166    -25.242244   -24.648881   -25.315466   \n",
       "75%      -24.000929    -24.242664    -24.399705   -23.807955   -24.462129   \n",
       "max      -19.795987    -17.653614    -18.349328   -16.823430   -15.609870   \n",
       "\n",
       "       ...    pmi__queen     pmi__king  pmi__queens   pmi__kings  \\\n",
       "count  ...  10119.000000  19446.000000  3313.000000  6617.000000   \n",
       "mean   ...    -25.361834    -25.395301   -24.835216   -24.729553   \n",
       "std    ...      1.435780      1.500407     1.723579     1.696653   \n",
       "min    ...    -30.485745    -31.237119   -28.636829   -29.793686   \n",
       "25%    ...    -26.331394    -26.407824   -25.942235   -25.915272   \n",
       "50%    ...    -25.429962    -25.440690   -25.287083   -24.966771   \n",
       "75%    ...    -24.524708    -24.469532   -24.229567   -23.792893   \n",
       "max    ...    -17.835727    -18.720616   -16.770424   -17.806201   \n",
       "\n",
       "       pmi__princess  pmi__prince  pmi__princesses  pmi__princes  \\\n",
       "count    5412.000000  8203.000000      1266.000000   3825.000000   \n",
       "mean      -25.019586   -25.328138       -23.698000    -23.932025   \n",
       "std         1.477100     1.462137         1.718849      1.715749   \n",
       "min       -30.302262   -31.337065       -30.067307    -29.995963   \n",
       "25%       -26.010199   -26.314027       -24.823475    -25.134534   \n",
       "50%       -25.088823   -25.347642       -24.085664    -24.178638   \n",
       "75%       -24.122755   -24.380551       -22.893234    -22.914827   \n",
       "max       -17.986234   -18.367298       -15.972031    -16.816626   \n",
       "\n",
       "             pmi__he      pmi__she  \n",
       "count  100828.000000  66891.000000  \n",
       "mean      -25.416424    -25.262707  \n",
       "std         1.514534      1.499545  \n",
       "min       -32.885207    -33.474327  \n",
       "25%       -26.219694    -26.079852  \n",
       "50%       -25.137871    -25.078310  \n",
       "75%       -24.351869    -24.212292  \n",
       "max       -20.899774    -21.228257  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"../data\"\n",
    "\n",
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/pmi_by_gendered_expressions.csv\", index_col=0)\n",
    "print(len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fd3299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('she', 'he') pmi-defined words: 65912\n",
      "('her', 'his') pmi-defined words: 75032\n",
      "('her', 'him') pmi-defined words: 62131\n",
      "('hers', 'his') pmi-defined words: 7536\n",
      "! Pair (grandmother, grandfather) doesn't exist...\n",
      "! Pair (grandma, grandpa) doesn't exist...\n",
      "! Pair (stepmother, stepfather) doesn't exist...\n",
      "! Pair (stepmom, stepdad) doesn't exist...\n",
      "('mother', 'father') pmi-defined words: 26121\n",
      "('mom', 'dad') pmi-defined words: 9150\n",
      "('aunt', 'uncle') pmi-defined words: 5380\n",
      "! Pair (aunts, uncles) doesn't exist...\n",
      "('mummy', 'daddy') pmi-defined words: 1255\n",
      "('sister', 'brother') pmi-defined words: 15727\n",
      "('sisters', 'brothers') pmi-defined words: 8049\n",
      "('daughter', 'son') pmi-defined words: 18721\n",
      "('daughters', 'sons') pmi-defined words: 7276\n",
      "('female', 'male') pmi-defined words: 28115\n",
      "! Pair (females, males) doesn't exist...\n",
      "! Pair (feminine, masculine) doesn't exist...\n",
      "('woman', 'man') pmi-defined words: 31857\n",
      "('women', 'men') pmi-defined words: 38861\n",
      "! Pair (madam, sir) doesn't exist...\n",
      "! Pair (matriarchy, patriarchy) doesn't exist...\n",
      "('girl', 'boy') pmi-defined words: 21067\n",
      "! Pair (lass, lad) doesn't exist...\n",
      "('girls', 'boys') pmi-defined words: 18633\n",
      "('girlfriend', 'boyfriend') pmi-defined words: 5944\n",
      "('girlfriends', 'boyfriends') pmi-defined words: 1103\n",
      "('wife', 'husband') pmi-defined words: 20403\n",
      "('wives', 'husbands') pmi-defined words: 4507\n",
      "('queen', 'king') pmi-defined words: 9517\n",
      "('queens', 'kings') pmi-defined words: 2667\n",
      "('princess', 'prince') pmi-defined words: 4818\n",
      "('princesses', 'princes') pmi-defined words: 1094\n",
      "! Pair (lady, lord) doesn't exist...\n",
      "! Pair (ladies, lords) doesn't exist...\n",
      "('she', 'he') pmi-defined words: 65912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pmi__she</th>\n",
       "      <th>pmi__he</th>\n",
       "      <th>pmi_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149350</th>\n",
       "      <td>worldviews</td>\n",
       "      <td>-26.607044</td>\n",
       "      <td>-25.767380</td>\n",
       "      <td>-0.839664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97697</th>\n",
       "      <td>overspend</td>\n",
       "      <td>-25.071416</td>\n",
       "      <td>-24.994815</td>\n",
       "      <td>-0.076601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26858</th>\n",
       "      <td>caricaturing</td>\n",
       "      <td>-24.687485</td>\n",
       "      <td>-24.163941</td>\n",
       "      <td>-0.523544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123998</th>\n",
       "      <td>slatted</td>\n",
       "      <td>-25.064006</td>\n",
       "      <td>-25.430709</td>\n",
       "      <td>0.366702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>attentions</td>\n",
       "      <td>-23.186299</td>\n",
       "      <td>-23.783683</td>\n",
       "      <td>0.597384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110979</th>\n",
       "      <td>rearward</td>\n",
       "      <td>-27.267943</td>\n",
       "      <td>-26.689328</td>\n",
       "      <td>-0.578615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>12153</td>\n",
       "      <td>-24.776008</td>\n",
       "      <td>-24.636996</td>\n",
       "      <td>-0.139012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25189</th>\n",
       "      <td>burping</td>\n",
       "      <td>-23.977679</td>\n",
       "      <td>-24.346041</td>\n",
       "      <td>0.368362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116091</th>\n",
       "      <td>roadblocks</td>\n",
       "      <td>-25.271944</td>\n",
       "      <td>-24.999363</td>\n",
       "      <td>-0.272581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124507</th>\n",
       "      <td>smellin</td>\n",
       "      <td>-23.624592</td>\n",
       "      <td>-24.394020</td>\n",
       "      <td>0.769428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142425</th>\n",
       "      <td>unveils</td>\n",
       "      <td>-24.664684</td>\n",
       "      <td>-24.554712</td>\n",
       "      <td>-0.109972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115290</th>\n",
       "      <td>revised</td>\n",
       "      <td>-26.326665</td>\n",
       "      <td>-25.680263</td>\n",
       "      <td>-0.646403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50484</th>\n",
       "      <td>eternal</td>\n",
       "      <td>-25.227859</td>\n",
       "      <td>-24.657463</td>\n",
       "      <td>-0.570395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70243</th>\n",
       "      <td>incandescent</td>\n",
       "      <td>-25.456303</td>\n",
       "      <td>-25.545763</td>\n",
       "      <td>0.089460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84353</th>\n",
       "      <td>marriages</td>\n",
       "      <td>-24.964321</td>\n",
       "      <td>-25.226408</td>\n",
       "      <td>0.262087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word   pmi__she    pmi__he  pmi_diff\n",
       "149350    worldviews -26.607044 -25.767380 -0.839664\n",
       "97697      overspend -25.071416 -24.994815 -0.076601\n",
       "26858   caricaturing -24.687485 -24.163941 -0.523544\n",
       "123998       slatted -25.064006 -25.430709  0.366702\n",
       "16674     attentions -23.186299 -23.783683  0.597384\n",
       "110979      rearward -27.267943 -26.689328 -0.578615\n",
       "2268           12153 -24.776008 -24.636996 -0.139012\n",
       "25189        burping -23.977679 -24.346041  0.368362\n",
       "116091    roadblocks -25.271944 -24.999363 -0.272581\n",
       "124507       smellin -23.624592 -24.394020  0.769428\n",
       "142425       unveils -24.664684 -24.554712 -0.109972\n",
       "115290       revised -26.326665 -25.680263 -0.646403\n",
       "50484        eternal -25.227859 -24.657463 -0.570395\n",
       "70243   incandescent -25.456303 -25.545763  0.089460\n",
       "84353      marriages -24.964321 -25.226408  0.262087"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we may want to perform some correlation with other gendered words\n",
    "# we also define the PMI diff between words and other gendered word pairs\n",
    "GENDER_PAIRS, GENDER_PAIRS_NUM_WORDS = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "# ----------------------------------------------------------------------------\n",
    "# compute PMI diff used in the main paper\n",
    "# ----------------------------------------------------------------------------\n",
    "# Most analysis will focus on the pmi_diff(she, he)\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a049",
   "metadata": {},
   "source": [
    "## 2. Loading data - Load model scores for the different datasets\n",
    "\n",
    "Say, PMI_DIFF(w, she, he), let us now compute the pmi of the words used for each of the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset names:\n",
      "  -> ['USE-5', 'Winobias', 'Winogender', 'USE-10', 'USE-20'] \n",
      " --------------------------------------------------------------\n",
      " Number of files per dataset \n",
      " -----------------------------------------------------------------\n",
      " ->  USE-5 28\n",
      " ->  Winobias 56\n",
      " ->  Winogender 28\n",
      " ->  USE-10 28\n",
      " ->  USE-20 28\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"..\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"USE-5\": glob.glob(f\"{BASE_DIR}/results-words5/final-results/*__scores__*.csv\"),\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winobias*__scores__*.csv\"),\n",
    "    \"Winogender\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winogender*__scores__*.csv\"),\n",
    "    # \"StereoSet\": glob.glob(f\"../results-baselines/final-results/*StereoSet*__scores__*.csv\"),\n",
    "    # We define this ordering so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"USE-10\": glob.glob(f\"{BASE_DIR}/results-words10/final-results/*__scores__*.csv\"),\n",
    "    \"USE-20\": glob.glob(f\"{BASE_DIR}/results-words20/final-results/*__scores__*.csv\"),\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES, \"\\n\", \"-\" * 62)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                                     Validation\n",
    "# ------------------------------------------------------------------------------\n",
    "# All datasets must have exact same number of files and ordered in the same way.\n",
    "for dataset1, dataset2 in itertools.product(DATASET_NAMES, DATASET_NAMES):\n",
    "    fps1 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset1]] \n",
    "    fps2 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset2]] \n",
    "    c1, c2 = Counter(fps1), Counter(fps2)\n",
    "    assert len(c1 & c2) == len(c1), f\"Validation failed for datasets: ({dataset1}, {dataset2})\"\n",
    "\n",
    "# !! Assumption: When scoring there was no change in the ordering of the templates and therefore\n",
    "# every time we load the filepaths, we will have exactly the same ordering for all files (regardless\n",
    "# of the scoring model).\n",
    "DATASET_2_FILEPATHS = {k: sorted(v) for k, v in DATASET_2_FILEPATHS.items()}\n",
    "\n",
    "print(\" Number of files per dataset\", \"\\n\", \"-\" * 65)\n",
    "for name, files in DATASET_2_FILEPATHS.items():\n",
    "    print(\" -> \", name, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd74374",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "Transform the datasets into the canonic form:\n",
    "\n",
    "1. Transform model name into its canonic form: Extract from filepath name and add it as a column to the dataset.\n",
    "3. Obtain information about model size: \n",
    "2. Obtain information about the interventions: Is the model trained on duplicated data (is_deduped=False) or non-duplicated data (is_deduped=True).\n",
    "3. Obtain information about whether the test sentence pair is natural (is_natural=True) or whether is unnatural for one of the variants in the pair (is_natural=False)\n",
    "4. Obtain information about the model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92a5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered -549 unnatural, removed from USE-5\n",
      "Number of unique 'likely_under' labels (should be 1): 2\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winobias\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered 0 unnatural, removed from Winogender\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -203 unnatural, removed from USE-10\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "Filtered -106 unnatural, removed from USE-20\n",
      "Number of unique 'likely_under' labels (should be 1): 1\n",
      "USE-5 28\n",
      "Winobias 28\n",
      "Winogender 28\n",
      "USE-10 28\n",
      "USE-20 28\n",
      "Evaluating 28 models:\n",
      " - Mistral-7B-v0.1\n",
      " - Mixtral-8x7B-v0.1\n",
      " - OLMo-1B\n",
      " - OLMo-7B\n",
      " - gpt-j-6b\n",
      " - llama-2-13b\n",
      " - llama-2-70b\n",
      " - llama-2-7b\n",
      " - mpt-30b\n",
      " - mpt-7b\n",
      " - opt-125m\n",
      " - opt-2.7b\n",
      " - opt-350m\n",
      " - opt-6.7b\n",
      " - pythia-1.4b\n",
      " - pythia-1.4b (D)\n",
      " - pythia-12b\n",
      " - pythia-12b (D)\n",
      " - pythia-160m\n",
      " - pythia-160m (D)\n",
      " - pythia-2.8b\n",
      " - pythia-2.8b (D)\n",
      " - pythia-410m\n",
      " - pythia-410m (D)\n",
      " - pythia-6.9b\n",
      " - pythia-6.9b (D)\n",
      " - pythia-70m\n",
      " - pythia-70m (D)\n"
     ]
    }
   ],
   "source": [
    "from model_utils import *\n",
    "\n",
    "# Mapping from dataset name to the file dataframes\n",
    "DATASET_2_FILES = defaultdict(list)\n",
    "\n",
    "# Read each individual filepath, creating an association <str, list<dataframe>>.\n",
    "# every str should have a list of the same size.\n",
    "\n",
    "# To test the impact of ommiting the unnaturalness check, CHANGE THE VALUE BELOW TO FALSE\n",
    "FILTER_UNNATURAL = True\n",
    "# ------------------------------ ------------------------------ ------------------------------\n",
    "DATASET_2_FILES = {\n",
    "    dataset: [read_filepath(fp, dataset, filter_unnatural=FILTER_UNNATURAL) for fp in sorted(fps)]\n",
    "    for dataset, fps in DATASET_2_FILEPATHS.items()\n",
    "}\n",
    "\n",
    "# Merge all the dataframes into a single big dataframe that contains the information of all models\n",
    "# for each dataset. We've created a original index to keep track of the unique sentences.\n",
    "# Sort the files per (model, orig_index)\n",
    "DATASET_2_FILES = {\n",
    "    dataset: pd.concat(dfs).sort_values([\"model\", \"orig_index\"]).reset_index(drop=True)\n",
    "    for dataset, dfs in DATASET_2_FILES.items()\n",
    "}\n",
    "\n",
    "# Number of models being evaluated \n",
    "NUM_EVAL_MODELS = []\n",
    "MODELS = []\n",
    "for dataset, df  in DATASET_2_FILES.items():\n",
    "    print(dataset, df[\"model\"].nunique())\n",
    "    MODELS.extend(df[\"model\"].unique())\n",
    "    NUM_EVAL_MODELS.append(df[\"model\"].nunique())\n",
    "    \n",
    "# We force the number of models to be the same across all datasets\n",
    "assert len(set(NUM_EVAL_MODELS)) == 1, \\\n",
    "    f\"Found various model sizes: {NUM_EVAL_MODELS}\"\n",
    "\n",
    "NUM_EVAL_MODELS = NUM_EVAL_MODELS[0]\n",
    "print(\"Evaluating\", NUM_EVAL_MODELS, \"models:\")\n",
    "MODELS = list(sorted(set(MODELS)))\n",
    "print(\" -\", \"\\n - \".join(MODELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776380d",
   "metadata": {},
   "source": [
    "## Post processing: \n",
    "\n",
    "In this section, we will carry some processing of the templates (column \"template\").\n",
    "\n",
    "\n",
    "**1. Remove placeholders from templates** : We first remove the placeholders (e.g., \"{PRONOUN}\", \"{PRONOUN1}\", \"{PRONOUN2}\", \"{PRONOUN2}self\") from the template.\n",
    "\n",
    "**2. Remove stopwords from the templates**: We use **spacy**'s stopwords except that we add back some of the pronouns, effectively following the approach in [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "\n",
    "**3. Parse each template**: We use **spacy** tokenizer since this was what was used by [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/). While NTLK is much faster, it doesn't group together words like \"self-care\", which is treated as single word by spacy tokenizer. Therefore, we've updated the script to consider the spacy tokenization. Applying it to the whole DATASET_2_FILES[dataset] will be too time-consuming, so we will apply to the first portion of the data and then concatenate it to the dataframe.\n",
    "\n",
    "\n",
    "### Filtering\n",
    "\n",
    "\n",
    "Before applying the processing, we will first obtain the top unique templates by focusing on the subset of data of the first listed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22e3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking slices for dataset: USE-5\n",
      "Checking slices for dataset: Winobias\n",
      "Checking slices for dataset: Winogender\n",
      "Checking slices for dataset: USE-10\n",
      "Checking slices for dataset: USE-20\n"
     ]
    }
   ],
   "source": [
    "# Validation (!sanity check)\n",
    "# When selecting a data slice from the big dataframe\n",
    "# we must guarantee that the sentences match to one another\n",
    "# (that is necessary because the remaining of the code is relying\n",
    "# on ordering of the dataframes)\n",
    "def check_slices(dataset: pd.DataFrame, data2files: dict, models: List[str]):\n",
    "    \"\"\"Check for the ordering of the rows in ``dataset`` correspond to the\n",
    "    ones in ``data2files``. Since the data2files are ordered by models,\n",
    "    we will focus on that.\"\"\"\n",
    "    slices = []\n",
    "    for model in models:\n",
    "        df = data2files[dataset]\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "        if len(slices) > 1:\n",
    "            assert np.array_equal(slices[-1][\"template\"].values, df[\"template\"].values)    \n",
    "        slices.append(df)\n",
    "        \n",
    "    \n",
    "for dataset in DATASET_NAMES:\n",
    "    print(\"Checking slices for dataset:\", dataset)\n",
    "    check_slices(dataset=dataset, data2files=DATASET_2_FILES, models=MODELS)\n",
    "    \n",
    "# -----------------------------------------------------------------------------\n",
    "# ^Note: if the check above does not throw an error, then it means that the\n",
    "# templates can stack up based on the model, so it's ok to apply the processing\n",
    "# to the first model and then create NUM_EVAL_MODEL copies of that and insert\n",
    "# in the dataframe!!\n",
    "# -----------------------------------------------------------------------------\n",
    "DATASET_2_TEMPLATES = {\n",
    "    dataset: df[df[\"model\"] == MODELS[0]][\"template\"].values.tolist()\n",
    "    for dataset, df in DATASET_2_FILES.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3857b",
   "metadata": {},
   "source": [
    "### Processing (using Spacy tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349a8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cbelem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/cbelem/projects/tokenization-proj/venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "import nltk \n",
    "import re, string\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "PRONOUNS = [\"she\", \"her\", \"hers\", \"he\", \"his\", \"him\", \"himself\", \"herself\"]\n",
    "SPACY_PARSER = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"tagger\"])\n",
    "\n",
    "\n",
    "def postprocess_spacy(templates, pronouns=PRONOUNS):\n",
    "    def word_tokenize(sentence: str, pronouns: list, remove_stopwords: bool=True, remove_punct: bool=True):\n",
    "        doc = SPACY_PARSER(sentence)\n",
    "        # Extract the tokens that are not stopwords\n",
    "        tokens = [token.text for token in doc \n",
    "                  if (token.text in pronouns) or (not token.is_stop and not token.is_punct)]\n",
    "        return [t for t in tokens if len(t.strip()) > 0]\n",
    "\n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Step 1. Remove placeholders from the templates\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    # Step 2. Parse the sentence\n",
    "    templates = [word_tokenize(t, pronouns) for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "def postprocess_nltk(templates, pronouns=PRONOUNS):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    # We know that some sentences have some other references to other entities,\n",
    "    # let's keep some pronouns\n",
    "    nltk_stopwords -= set(pronouns)\n",
    "    punct = string.punctuation\n",
    "    \n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Remove pronouns first\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    \n",
    "    # Remove stopwords and punct\n",
    "    templates = [[w for w in word_tokenize(t) if w not in punct and w not in nltk_stopwords] for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "DATASET_2_CANONIC_TEMPLATES_SPACY = {}\n",
    "DATASET_2_CANONIC_TEMPLATES_NLTK = {}\n",
    "\n",
    "for dataset, templates in DATASET_2_TEMPLATES.items():\n",
    "    DATASET_2_CANONIC_TEMPLATES_SPACY[dataset] = postprocess_spacy(templates)\n",
    "    DATASET_2_CANONIC_TEMPLATES_NLTK[dataset] = postprocess_nltk(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516756",
   "metadata": {},
   "source": [
    "## Determine gender co-occurrence for each word \n",
    "\n",
    "In this section, we iterate the templates and compute the gender co-occurrence values for each sentence in the benchmarks. Optionally, you can weight the values of each word by the likelihood of being toxic or having negative sentiment. If such values are not provided, we assume each word is worth the same value of 1 unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415b1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dataframe to mapping from word to pmi diff for easy access\n",
    "WORD2PMI = PMI_DIFF[[\"word\", \"pmi_diff\"]].set_index(\"word\").to_dict()[\"pmi_diff\"]\n",
    "WORD2WEIGHTS = defaultdict(lambda: 1)\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on frequency\n",
    "## ----------------------------------------------------------------\n",
    "FREQ_WORDS = pd.read_csv(\"../data/pmi_file_w_counts.csv\", index_col=0)\n",
    "FREQ_WORDS[\"log_freq\"] = np.log(FREQ_WORDS[\"freq\"])\n",
    "\n",
    "## uncomment one of the lines below if you prefer weighting each word based\n",
    "## on the frequency of each individual word\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"freq\"]].set_index(\"word\").to_dict()[\"freq\"]\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"log_freq\"]].set_index(\"word\").to_dict()[\"log_freq\"]\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on toxicity/sentiment\n",
    "## ----------------------------------------------------------------\n",
    "## TODO:\n",
    "## -> Define toxicity for each word\n",
    "## -> Define sentiment polarity for each word (?)\n",
    "## Define a 1-to-1 mapping and assign the variable WORD2WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06df4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi_diff_per_sentences(\n",
    "        templates: List[List[str]],\n",
    "        word2pmi: dict,\n",
    "        word2weights: dict,\n",
    "    ) -> List[List[float]]:\n",
    "    \"\"\"Computes the PMI difference per individual token in the provided sentences.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    It assumes the templates/sentences are already provided as a list of tokens.\n",
    "    It returns two lists: the first one contains the list of pmi values for each of\n",
    "    the provided words (some tokens won't have a PMI value associated); the second\n",
    "    list contains the 1-1 mapping from word to pmi value and their weights.\n",
    "    \"\"\"\n",
    "    pmi_values = []\n",
    "    words_with_pmi = []\n",
    "    \n",
    "    for template in templates:\n",
    "        pmi = np.array([word2weights[w] * word2pmi.get(w) for w in template if word2pmi.get(w) is not None])\n",
    "        pmiwords = [{\n",
    "            \"word\": w, \n",
    "            \"pmi\": round(word2pmi.get(w), 2),\n",
    "            \"weight\": round(word2weights[w], 2),\n",
    "        } for w in template if word2pmi.get(w) is not None]\n",
    "        \n",
    "        pmi_values.append(pmi)\n",
    "        words_with_pmi.append(pmiwords)\n",
    "            \n",
    "    return pmi_values, words_with_pmi\n",
    "        \n",
    "\n",
    "PMI_PER_SENTENCES_NLTK = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_NLTK.items()\n",
    "}\n",
    "\n",
    "PMI_PER_SENTENCES_SPACY = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_SPACY.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037bd55",
   "metadata": {},
   "source": [
    "Since in general **spacy** tokenizer leads to higher pct of examples being matched with a word. We will use the **spacy** tokenized templates to conduct the analysis (it increases the coverage of the constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_PER_TEMPLATES = {}\n",
    "PMIWORDS_PER_TEMPLATES = {}\n",
    "\n",
    "# Change the PMI_PER_SENTENCES_SPACY with PMI_PER_SENTENCES_NLTK\n",
    "# to use NLTK tokenization instead.\n",
    "# for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_NLTK.items():\n",
    "for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_SPACY.items():\n",
    "    pmi_vals, words_per_pmi = pmi_per_sents_values\n",
    "    \n",
    "    PMI_PER_TEMPLATES[dataset] = pmi_vals\n",
    "    PMIWORDS_PER_TEMPLATES[dataset] = words_per_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6423c5",
   "metadata": {},
   "source": [
    "### Compute the constraint: MaxPMI(s)\n",
    "\n",
    "In this section, we compute the max gender PMI value per sentence. This consists of determining that's the max absolute word-level PMI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa5b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "\n",
    "def max_gender_pmi(templates_pmi: List[List[str]], col: str) -> List[dict]:\n",
    "    \"\"\"Compute the maximum PMI diff per sentence.\"\"\"\n",
    "    def _max_pmi(lst_pmis: List[str]) -> float:\n",
    "        if len(lst_pmis) > 0:\n",
    "            idx = np.argmax(np.abs(lst_pmis))\n",
    "            return lst_pmis[idx]\n",
    "    \n",
    "    results = []\n",
    "    for template_pmi in templates_pmi:\n",
    "        max_val = _max_pmi(template_pmi)\n",
    "        results.append({col: max_val, f\"{col}_invalid\": max_val is None, \"template_words_pmi\": template_pmi})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4917c59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['orig_index', 'word', 'target_word', 'sentence', 'has_placeholder',\n",
       "       'template', 'modifications', 'likely_under', 'is_natural', 'has_word',\n",
       "       'is_revised', 'M_num_tokens', 'M_logprob', 'M_template', 'F_num_tokens',\n",
       "       'F_logprob', 'F_template', 'FM_logprob', 'model', 'dataset',\n",
       "       'is_deduped', 'is_intervention', 'orig_model_name', 'model_size',\n",
       "       'model_family', 'max_gender_pmi', 'max_gender_pmi_invalid',\n",
       "       'template_words_pmi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contains the max gender pmi values per sentence\n",
    "MAX_GENDER_PMI = {dataset: max_gender_pmi(templates_pmi, MAXGENDER_COL) \n",
    "                  for dataset, templates_pmi in PMI_PER_TEMPLATES.items()}\n",
    "\n",
    "MAX_GENDER_PMI_LONG = []\n",
    "for dataset, lst_value_dicts in MAX_GENDER_PMI.items():\n",
    "    for value_dict in lst_value_dicts:\n",
    "        r = {k: v for k, v in value_dict.items()}\n",
    "        r[\"dataset\"] = dataset\n",
    "        MAX_GENDER_PMI_LONG.append(r)\n",
    "\n",
    "MAX_GENDER_PMI_LONG = pd.DataFrame(MAX_GENDER_PMI_LONG)\n",
    "        \n",
    "# Adds the information to the original dataset with all models\n",
    "# originally, preserved in the variable DATASET_2_FILES\n",
    "DATASET_W_CONSTRAINTS = {dataset: pd.DataFrame(values * NUM_EVAL_MODELS)\n",
    "                  for dataset, values in MAX_GENDER_PMI.items()}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# \n",
    "#                        Dataset w/ MaxGender PMI constraint!\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "DATASET_W_CONSTRAINTS = {\n",
    "    dataset: pd.concat((DATASET_2_FILES[dataset], DATASET_W_CONSTRAINTS[dataset]), copy=True, axis=1)\n",
    "    for dataset in DATASET_NAMES\n",
    "}\n",
    "\n",
    "DATASET_W_CONSTRAINTS[DATASET_NAMES[0]].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477d976-d26b-4841-ac6c-98d6bb7b84d8",
   "metadata": {},
   "source": [
    "## Persist post-processed results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da7349-ea89-43ce-ab97-644c576d6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df in DATASET_W_CONSTRAINTS.items():\n",
    "    df.to_csv(f\"../results/{dataset}-no-maxpmi-constraint.csv.gz\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
