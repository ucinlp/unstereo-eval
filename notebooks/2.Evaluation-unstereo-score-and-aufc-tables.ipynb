{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CAMERA-READY PLOTTING (thanks Alex Boyd!)\n",
    "# -----------------------------------------------------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "# The following code is borrowed from material provided by Alex!\n",
    "FULL_WIDTH = 5.50107\n",
    "COL_WIDTH  = 4.50461\n",
    "\n",
    "# Accessibility\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "matplotlib.rcParams[\"axes.prop_cycle\"] = matplotlib.cycler(color=sns.color_palette(\"colorblind\"))\n",
    "\n",
    "# Put at top of plotting script (requires tex be installed though)\n",
    "matplotlib.rc('font', family='serif', size=20)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def adjust(fig, left=0.0, right=1.0, bottom=0.0, top=1.0, wspace=0.0, hspace=0.0):\n",
    "    fig.subplots_adjust(\n",
    "        left   = left,  # the left side of the subplots of the figure\n",
    "        right  = right,  # the right side of the subplots of the figure\n",
    "        bottom = bottom,  # the bottom of the subplots of the figure\n",
    "        top    = top,  # the top of the subplots of the figure\n",
    "        wspace = wspace,  # the amount of width reserved for blank space between subplots\n",
    "        hspace = hspace,  # the amount of height reserved for white space between subplots\n",
    "    )\n",
    "    \n",
    "def save_fig(fig, name, **kwargs):\n",
    "    basedir = os.makedirs(\"./camera_ready/images\", exist_ok=True)\n",
    "    fig.savefig(f\"./camera_ready/images/{name}.pdf\", bbox_inches=\"tight\", **kwargs)\n",
    "\n",
    "def disable_axis(ax):\n",
    "    ax.set_zorder(-100)  # Avoids a visual rendering bug\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.setp(ax.spines.values(), color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Load model files\n",
    "\n",
    "Run `post-process-results.ipynb` first to generate a compiled version of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"USE-5\": f\"{RESULTS_DIR}/USE-5-no-maxpmi-constraint.csv.gz\",\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": f\"{RESULTS_DIR}/Winobias-no-maxpmi-constraint.csv.gz\",\n",
    "    \"Winogender\": f\"{RESULTS_DIR}/Winogender-no-maxpmi-constraint.csv.gz\",\n",
    "    # We define this ordering so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"USE-10\": f\"{RESULTS_DIR}/USE-10-no-maxpmi-constraint.csv.gz\",\n",
    "    \"USE-20\": f\"{RESULTS_DIR}/USE-20-no-maxpmi-constraint.csv.gz\",\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES, \"\\n\")\n",
    "\n",
    "# Read each individual filepath, creating an association <str, dataframe>.\n",
    "# every str should have a list of the same size.\n",
    "DATASET_2_FILES = {name: pd.read_csv(fp) for name, fp in DATASET_2_FILEPATHS.items()}\n",
    "DATASET_2_FILES = {name: df.sort_values([\"model\", \"orig_index\"]).reset_index(drop=True) for name, df in DATASET_2_FILES.items()}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Determine whether the number of evaluated models are the same\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "MODELS, NUM_EVAL_MODELS = [], []\n",
    "\n",
    "for dataset, df in DATASET_2_FILES.items():\n",
    "    print(\"Number of evaluated models for dataset\", dataset, \"is\", df[\"model\"].nunique())\n",
    "    MODELS.extend(df[\"model\"].unique())\n",
    "    NUM_EVAL_MODELS.append(df[\"model\"].nunique())\n",
    "    \n",
    "# We force the number of models to be the same across all datasets\n",
    "if len(set(NUM_EVAL_MODELS)) != 1:\n",
    "    warnings.warn(f\"Inconsistent number of models across the different evaluation mber models: {NUM_EVAL_MODELS}\")\n",
    "\n",
    "NUM_EVAL_MODELS = NUM_EVAL_MODELS[0]\n",
    "print(\"Evaluating\", NUM_EVAL_MODELS, \"models:\")\n",
    "MODELS = list(sorted(set(MODELS)))\n",
    "print(\" -\", \"\\n - \".join(MODELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Validation (!sanity check)\n",
    "# ------------------------------------------------------------------------\n",
    "# When selecting a data slice from the big dataframe\n",
    "# we must guarantee that the sentences match to one another\n",
    "# (that is necessary because the remaining of the code is relying\n",
    "# on ordering of the dataframes)\n",
    "def check_slices(dataset: pd.DataFrame, data2files: dict, models: List[str]):\n",
    "    \"\"\"Check for the ordering of the rows in ``dataset`` correspond to the\n",
    "    ones in ``data2files``. Since the data2files are ordered by models,\n",
    "    we will focus on that.\"\"\"\n",
    "    slices = []\n",
    "    for model in models:\n",
    "        df = data2files[dataset]\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "        if len(slices) > 1:\n",
    "            assert np.array_equal(slices[-1][\"template\"].values, df[\"template\"].values)    \n",
    "        slices.append(df)\n",
    "        \n",
    "    \n",
    "for dataset in DATASET_NAMES:\n",
    "    print(\"Checking slices for dataset:\", dataset)\n",
    "    check_slices(dataset=dataset, data2files=DATASET_2_FILES, models=MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3028b",
   "metadata": {},
   "source": [
    "## Data Analysis - Filtering using $\\eta$\n",
    "\n",
    "In this section, we observe how the number of templates changes as we increase the max gender pmi difference. We observe that little to no evaluation examples remain after enforcing smaller values of $\\mathrm{MaxPMI(s)}$. Conversely, as we relax the constraint, more and more examples are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import filter_eta_and_count_examples\n",
    "\n",
    "\n",
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "FILTERING_ETA = np.linspace(0.0, 2.5, 101)[::-1]\n",
    "print(\"Processing column\", MAXGENDER_COL, \"for values\", FILTERING_ETA)\n",
    "\n",
    "FILTER_CURVES_RESULTS = filter_eta_and_count_examples(\n",
    "    name_and_dataset=DATASET_2_FILES,\n",
    "    etas=FILTERING_ETA,\n",
    "    col=MAXGENDER_COL,\n",
    "    constant=NUM_EVAL_MODELS, \n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(FULL_WIDTH, FULL_WIDTH*2/3))\n",
    "sns.lineplot(FILTER_CURVES_RESULTS, x=\"filter\", y=\"freq\", hue=\"dataset\", lw=2) #set y=\"counts\" to plot absolute values instead\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"$\\eta$\")\n",
    "ax.set_ylabel(\"Percentage of Dataset\")\n",
    "ax.legend(title=\"Dataset\", loc=\"upper left\", bbox_to_anchor=(0.56, 0.70))\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.20))\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "# Add grid\n",
    "ax.grid(axis='x', which='major', linewidth=1, linestyle=\":\", color=\"lightgray\")\n",
    "ax.grid(axis='y', which=\"major\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim((0, 2))\n",
    "ax.set_ylim((0, 1))\n",
    "adjust(fig)\n",
    "save_fig(fig, \"lineplot__datasetpct_vs_maxpmi\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe331f",
   "metadata": {},
   "source": [
    "## Fairness metrics - Fixed threshold & AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "# fairness col in natural log space\n",
    "FAIRNESS_COL = \"FM_logprob\"\n",
    "\n",
    "# probability space threshold\n",
    "_FAIRNESS_THRESHOLD = 1.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0409c",
   "metadata": {},
   "source": [
    "**Natural logarithm base**: To report the results in natural logarithm, use the following cell. \n",
    "While earlier versions of the paper included the natural logarithm results, in the camera ready version of the paper, we decided to use the **base 10** since it is more intuitive and easy to reason about."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42271608",
   "metadata": {},
   "source": [
    "FAIRNESS_THRESHOLD = np.log(_FAIRNESS_THRESHOLD)\n",
    "FAIRNESS_EPSILONS = np.linspace(0, 10, 101)\n",
    "MAX_AUC = 10\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fbb2c",
   "metadata": {},
   "source": [
    "**Base 10 logarithm**: To report the results for the camera ready version of the paper, we use the base 10, since it makes it easier to think about the meaning of the value in the plots. We stick to the default value of 1.65, such that the results found in earlier versions of the paper (eg, [paper at the NeurIPS SOLAR workshop in 2023](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=nMwgV2UAAAAJ&sortby=pubdate&citation_for_view=nMwgV2UAAAAJ:_kc_bZDykSQC)) can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH, FULL_WIDTH))\n",
    "sns.boxplot(FAIR_AUC, y=\"dataset\", x=\"auc\", ax=ax)\n",
    "ax.axvline(MAX_AUC, ls=\"--\", color=\"black\", label=\"max auc\")\n",
    "ax.set_ylabel(\"Dataset\")\n",
    "ax.set_xlabel(\"Area under the fairness curve\")\n",
    "ax.spines[['right', 'top']].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a9c42",
   "metadata": {},
   "source": [
    "### Fairness AUC (discriminated by the different fairness thresholds)\n",
    "\n",
    "\n",
    "The following table represents the AuFC measure for the different filtering values that we used to compute the AuFC. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the long table into a wide table, by extending it with the dataset names\n",
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "pd.pivot_table(FAIR_AUC, index=\"model\", values=[\"auc\"], columns=[\"dataset_\"]).style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_threshold_plots(fairthresholds, fairauc, datasetnames, models, use_exp=None):\n",
    "    models, tag = models[0], models[1]\n",
    "    \n",
    "    # For every dataset create a plot\n",
    "    for dataset in datasetnames:\n",
    "        # Obtain the subset corresponding to the desired dataset\n",
    "        ft_df = fairthresholds[fairthresholds[\"dataset\"] == dataset].copy()\n",
    "        \n",
    "        # Plot only the specified models\n",
    "        ft_df = ft_df[ft_df[\"model\"].isin(models)]\n",
    "        \n",
    "        # Obtain the AUC for that model and dataset\n",
    "        aucs = fairauc[(fairauc[\"dataset\"] == dataset) & (fairauc[\"model\"].isin(models))]\n",
    "        \n",
    "        ft_df[\"Deduplicated\"] = ft_df[\"model\"].apply(lambda x: \"(D)\" in x)\n",
    "        ft_df[\"Model\"] = ft_df[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "            \n",
    "        if all([\"pythia\" in m for m in models]):\n",
    "            ft_df[\"Model\"] = ft_df[\"Model\"].apply(lambda x: x.replace(\"pythia-\", \"\"))\n",
    "        \n",
    "        if dataset in (\"Winobias\", \"Winogender\"):\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH/2, 2))\n",
    "            ax.set_xlim((0, 5))\n",
    "\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH/2, 2))\n",
    "            ax.set_xlim((0, 5))\n",
    "\n",
    "            \n",
    "        adjust(fig)\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "        if use_exp is not None:\n",
    "            ft_df[\"fairness_eps\"] = ft_df[\"fairness_eps\"].apply(use_exp)\n",
    "\n",
    "        # Plot one line per model\n",
    "        # Plot one line using different stule but same color if the model is deduplicated\n",
    "        \n",
    "        if ft_df[\"Deduplicated\"].nunique() > 1:\n",
    "            kwargs = dict(style=\"Deduplicated\")\n",
    "        else:\n",
    "            kwargs = dict()\n",
    "        \n",
    "        sns.lineplot(ft_df, x=\"fairness_eps\", y=\"pct_examples\", hue=\"Model\", lw=2, ax=ax, **kwargs)\n",
    "        # ax.axvline(FAIRNESS_THRESHOLD, color=\"black\", alpha=0.5)\n",
    "        ax.set_title(dataset, fontsize=12)\n",
    "        ax.set_xlabel(\"threshold\", fontsize=12)\n",
    "        ax.set_ylabel(\"fairness metric\", fontsize=12)\n",
    "        ax.set_ylim((0, 1))\n",
    "        \n",
    "        ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(0.20))\n",
    "\n",
    "        # Add axis formatting\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "\n",
    "        ax.grid(axis='x', which=\"major\", linewidth=1, linestyle='--', color=\"lightgray\")\n",
    "        ax.grid(axis='x', which=\"minor\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "        \n",
    "        # Legend\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(0.5, 0.9), fontsize=12)\n",
    "        save_fig(fig, f\"lineplot__{dataset}_{tag}_in_func_eps\", dpi=100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792fc8d",
   "metadata": {},
   "source": [
    "### AuFC: Pythia models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_models = [\n",
    "    'pythia-70m',\n",
    "    'pythia-70m (D)',\n",
    "    # 'pythia-2.8b',\n",
    "    # 'pythia-2.8b (D)',\n",
    "    'pythia-6.9b',\n",
    "    'pythia-6.9b (D)',\n",
    "    'pythia-12b',\n",
    "    'pythia-12b (D)',\n",
    "    # 'gpt-j-6b'\n",
    "], \"pythia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753aac9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment expression below if you want to plot the x axis in the probability space\n",
    "# (it assumes that fair thresholds and fair auc were previously computed in the log 10.)\n",
    "# fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models, use_exp=lambda x: 10**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c3185",
   "metadata": {},
   "source": [
    "### AuFC: OPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692fcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_models = [\n",
    "    'opt-125m',\n",
    "    'opt-2.7b',\n",
    "    'opt-350m',\n",
    "    'opt-6.7b',\n",
    "], \"opt\"\n",
    "\n",
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, opt_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df4237",
   "metadata": {},
   "source": [
    "### AuFC: mpt * llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e9e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "misc_models = [\n",
    "    'llama-2-13b',\n",
    "    'llama-2-7b',\n",
    "    'llama-2-70b',\n",
    "    'mpt-30b',\n",
    "    'mpt-7b',\n",
    "    \"OLMo-1B\",\n",
    "    \"OLMo-7B\",\n",
    "    \"Mistral-7B-v0.1\",\n",
    "    \"Mixtral-8x7B-v0.1\",\n",
    "], \"others\"\n",
    "\n",
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, misc_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157085f",
   "metadata": {},
   "source": [
    "Let us create the grid for the fairness threshold picture in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def individual_fairness_threshold_plot(fairthresholds, fairauc, dataset, models, max_auc, ax, use_exp=None, simplify=True):\n",
    "    # Obtain the subset corresponding to the desired dataset\n",
    "    ft_df = fairthresholds[fairthresholds[\"dataset\"] == dataset].copy()\n",
    "\n",
    "    # Plot only the specified models\n",
    "    ft_df = ft_df[ft_df[\"model\"].isin(models)]\n",
    "\n",
    "    # Obtain the AUC for that model and dataset\n",
    "    aucs = fairauc[(fairauc[\"dataset\"] == dataset) & (fairauc[\"model\"].isin(models))]\n",
    "\n",
    "    ft_df[\"Original\"] = ft_df[\"model\"].apply(lambda x: \"No\" if \"(D)\" in x else \"Yes\")\n",
    "    ft_df[\"Model\"] = ft_df[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "    \n",
    "    if simplify and all([\"pythia\" in m for m in models]):\n",
    "        ft_df[\"Model\"] = ft_df[\"Model\"].apply(lambda x: x.replace(\"pythia-\", \"\"))\n",
    "\n",
    "    if use_exp is not None:\n",
    "        ft_df[\"fairness_eps\"] = ft_df[\"fairness_eps\"].apply(use_exp)\n",
    "\n",
    "    \n",
    "    kwargs = {\"style\": \"Original\"} if ft_df[\"Original\"].nunique() > 1 else {}    \n",
    "    sns.lineplot(ft_df, x=\"fairness_eps\", y=\"pct_examples\", hue=\"Model\", lw=1, ax=ax, alpha=0.8, **kwargs)\n",
    "    # ax.axvline(FAIRNESS_THRESHOLD, color=\"black\", alpha=0.5)\n",
    "    ax.set_title(dataset, fontsize=15)\n",
    "    ax.set_xlabel(\"threshold\")\n",
    "    ax.set_ylabel(\"fairness metric\")\n",
    "    ax.set_xlim((0, max_auc))\n",
    "    ax.set_ylim((0, 1))\n",
    "\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.25))\n",
    "\n",
    "    # Add axis formatting\n",
    "    # ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "\n",
    "    ax.grid(axis='x', which=\"major\", linewidth=1, linestyle='--', color=\"lightgray\")\n",
    "    # ax.grid(axis='x', which=\"minor\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(0.40, 0.75), fontsize=12)\n",
    "    \n",
    "    \n",
    "# Separate plotting the data from formatting the figure\n",
    "def plot_results_fairness(ax, name, **kwargs):\n",
    "    if name == \"USE-5\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-5\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "\n",
    "    elif name == \"Winobias\":\n",
    "        individual_fairness_threshold_plot(dataset=\"Winobias\", ax=ax, max_auc=3, **kwargs)\n",
    "    elif name == \"Winogender\":\n",
    "        individual_fairness_threshold_plot(dataset=\"Winogender\", ax=ax, max_auc=3, **kwargs)\n",
    "    elif name == \"USE-10\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-10\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "    elif name == \"USE-20\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-20\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "    else:\n",
    "        raise NotImplemented(f\"Unexpected plot: {name}\")\n",
    "\n",
    "    \n",
    "def make_figure(is_horizontal, plot_results, dataset_names, models, **kwargs):\n",
    "    models, tag = models\n",
    "    if is_horizontal:\n",
    "        \n",
    "        mosaic = []\n",
    "        width_ratios = []\n",
    "        for name in dataset_names:\n",
    "            mosaic.append(name); width_ratios.append(1)\n",
    "            mosaic.append(\".\"); width_ratios.append(0.2)\n",
    "            \n",
    "        if len(dataset_names) == 5:\n",
    "            width_ratios = [1, 0.2, 0.75, 0.2, 0.75, 0.2, 1, 0.2, 1, 0.2]\n",
    "            \n",
    "        fig, axd = plt.subplot_mosaic(\n",
    "            mosaic=[mosaic[:-1]],\n",
    "            gridspec_kw={\"width_ratios\": width_ratios[:-1]},\n",
    "            figsize=(FULL_WIDTH, 2),\n",
    "            sharey=True,\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        AB_gap, BC_gap = 0.2, 0.2\n",
    "\n",
    "        fig, axd = plt.subplot_mosaic(\n",
    "            mosaic=[\n",
    "                [\"A\"], \n",
    "                ['.'], \n",
    "                [\"B\"], \n",
    "                ['.'],\n",
    "                [\"C\"],\n",
    "            ],\n",
    "            gridspec_kw={\"height_ratios\": [1, AB_gap, 1, BC_gap, 1]},\n",
    "            figsize=(2, FULL_WIDTH),\n",
    "            sharey=True,\n",
    "        )\n",
    "    \n",
    "\n",
    "    adjust(fig)\n",
    "    \n",
    "    for name, ax in axd.items():\n",
    "        plot_results(ax, name, models=models,**kwargs)\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.set_xlabel(\"threshold\", fontsize=14)\n",
    "        ax.set_ylabel(\"fairness metric\", fontsize=14)\n",
    "\n",
    "        \n",
    "        if ax != axd[dataset_names[-1]]:\n",
    "            ax.legend([],[], frameon=False)\n",
    "        else:\n",
    "            ax.legend(loc=\"upper center\", ncol=1, bbox_to_anchor=(0.8, 0.95), fontsize=12)\n",
    "\n",
    "    return fig, tag\n",
    "\n",
    "\n",
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES,#\n",
    "                #dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-2:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots5__{tag}__fairness_metric_in_func_eps\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e772f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-3:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots4__ours__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-2:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots3_ours__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d00281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[1:3], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots2_others__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825993a0",
   "metadata": {},
   "source": [
    "## Fairness Neutrality, Unstereo Score (US)\n",
    "\n",
    "In this section, we aim to compute the different skews of the models for various constrained settings. \n",
    "In particular, we will compute:\n",
    "\n",
    "1. **Fairness metric**: focus on the computation of the neutral examples, i.e., the examples whose test sentence pair likelihoods are within $\\exp^{\\epsilon_f}$\n",
    "2. Difference in predicted female vs predicted male: if the sentences are not being predicted neutral, how is the model assigning the probability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD, FAIRNESS_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print(f\"Using threshold: {FAIRNESS_THRESHOLD:.4f} to compute fairness metric\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Original dataset (before applying any of the max pmi constraints)\n",
    "BEFORE_FILTER = {dataset: df.copy() for dataset, df in DATASET_2_FILES.items()}\n",
    "\n",
    "# Use this version to use the natural logarithm\n",
    "# BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, 0.5)\n",
    "# use this version to use the base 10 results\n",
    "BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c65325",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_FILTER[\"USE-5\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5375ec",
   "metadata": {},
   "source": [
    "### Neutrality and AuFC (per constrained setting)\n",
    "\n",
    "While we propose a pipeline to create benchmarks that satisfy the gender co-occurrence constraints, in our experiments we do not immediately restrict our benchmarks. The main goal being that we'd like to be able to study the effect of stricter PMI constraints. For that reason, in the following setting, we will compute the value of Neutrality and AuFC for $\\eta \\in \\{0.3, 0.5, 0.65, 0.8, 1\\}$. The stricter setup being $\\eta = 0.3$ and the least strict being $\\eta = 1$. The original unconstrained version of the dataset (stored in variable `BEFORE_FILTER[<dataset>]`) is denoted $\\eta = \\infty$ in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1053999",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_THRESHOLDS = [0.3, 0.5, 0.65, 0.8, 1.0]\n",
    "\n",
    "print(f\"Fairness col: '{FAIRNESS_COL}' and threshold: '{FAIRNESS_THRESHOLD}'\")\n",
    "AFTER_FILTER = {}\n",
    "# Filter out the dataset_w_constraints according to the different PMI thresholds (or \\epsilon_k)\n",
    "for pmi_threshold in PMI_THRESHOLDS:\n",
    "    # Create the different filters for each dataset\n",
    "    print(\"eta =\", pmi_threshold)\n",
    "    AFTER_FILTER[pmi_threshold] = {\n",
    "        dataset: filter_data_by_col_val(df.copy(), col=MAXGENDER_COL, thres=pmi_threshold).copy()\n",
    "        for dataset, df in BEFORE_FILTER.items()\n",
    "    } \n",
    "\n",
    "# For each filtered version of the dataset, compute the corresponding skews and metrics\n",
    "AFTER_FILTER = {\n",
    "    filt: compute_skews_(bias_files, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base) for filt, bias_files in AFTER_FILTER.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results(data2files) -> pd.DataFrame:\n",
    "    return pd.merge(\n",
    "        # Compute unstereo score\n",
    "        compute_neutral_pct_w_std(data2files), \n",
    "        # Compute predictive disparity metric\n",
    "        compute_female_male_skews(data2files, MODELS),\n",
    "        on=[\"dataset\", \"model\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "METRICS_BEFORE_FILTER = merge_results(BEFORE_FILTER)\n",
    "METRICS_AFTER_FILTER = {eta: merge_results(AFTER_FILTER[eta]) for eta in AFTER_FILTER.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91dbbc",
   "metadata": {},
   "source": [
    "#### Number of examples before and after the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All examples:\")\n",
    "print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in BEFORE_FILTER.items()})\n",
    "\n",
    "\n",
    "for eps, eps_values in AFTER_FILTER.items():\n",
    "    print()\n",
    "    print(\"Number of examples after filter\", eps)\n",
    "    print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in eps_values.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149977",
   "metadata": {},
   "source": [
    "### Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def model2latex(model: str):    \n",
    "    if \"pythia\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"pythia-(.+)\", r\"pyths{\\1}\", model)\n",
    "    elif \"opt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"opt-(.+)\", r\"opts{\\1}\", model)\n",
    "    elif \"mpt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"mpt-(.+)\", r\"mpts{\\1}\", model)\n",
    "    elif \"llama-2\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"llama-2-(.+)\", r\"llamas{\\1}\", model)\n",
    "    elif \"gpt-j\" in model:\n",
    "        return \"\\\\\" + \"gptj\"\n",
    "    else:\n",
    "        return model\n",
    "        \n",
    "\n",
    "def print_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    print(table.set_index(\"model\").to_latex())\n",
    "\n",
    "    \n",
    "def get_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    return table.set_index(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b575d",
   "metadata": {},
   "source": [
    "### Neutral fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"NO FILTER\")\n",
    "print(\"\\n\", \"-\" * 80, \"\\n\\n\")\n",
    "print_results(METRICS_BEFORE_FILTER, \"neutral_final\")\n",
    "\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print_results(METRICS_AFTER_FILTER[eps], \"neutral_final\")\n",
    "    print(\"-\" * 80, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16481888",
   "metadata": {},
   "source": [
    "### Create tables w/ fairness gap\n",
    "\n",
    "\n",
    "#### Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NO FILTER\")\n",
    "r = get_results(METRICS_BEFORE_FILTER, \"neutral_avg\")\n",
    "r.to_csv(\"camera_ready/table/neutral_avg__unfiltered.csv\")\n",
    "fairness_gap_tables = {\"unfiltered\": r}\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    r = get_results(METRICS_AFTER_FILTER[eps], \"neutral_avg\")\n",
    "    r.to_csv(f\"camera_ready/table/neutral_avg__filtered__{eps}.csv\")\n",
    "    fairness_gap_tables[eps] = r\n",
    "    \n",
    "    \n",
    "orig = fairness_gap_tables[\"unfiltered\"]\n",
    "delta_08 = fairness_gap_tables[0.8] - orig\n",
    "delta_065 = fairness_gap_tables[0.65] - orig\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df.index = orig.index\n",
    "assert all(df.index == delta_08.index)\n",
    "assert all(df.index == delta_065.index)\n",
    "\n",
    "for dataset in [\"USE-05\", \"Winobias\", \"Winogender\"]:\n",
    "    df.insert(len(df.columns), f\"{dataset}__Orig\", orig[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.8}\", delta_08[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.65}\", delta_065[dataset])\n",
    "    \n",
    "print(df.style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9014fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.index = orig.index\n",
    "assert all(df.index == delta_08.index)\n",
    "assert all(df.index == delta_065.index)\n",
    "\n",
    "for dataset in [\"USE-10\", \"USE-20\"]:\n",
    "    df.insert(len(df.columns), f\"{dataset}__Orig\", orig[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.8}\", delta_08[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.65}\", delta_065[dataset])\n",
    "    \n",
    "print(df.style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efea9cb",
   "metadata": {},
   "source": [
    "#### Table 2. Impact of training data deduplication at $\\eta = 0.65$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cebfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.65\n",
    "tab2 = fairness_gap_tables[eta].reset_index().copy()\n",
    "tab2 = tab2[tab2[\"model\"].apply(lambda s: s.startswith(\"\\pyths\"))]\n",
    "\n",
    "tab2_dedup_mask = tab2[\"model\"].apply(lambda s: '(D)' in s)\n",
    "# original models\n",
    "tab2_orig = tab2[~tab2_dedup_mask].sort_values(\"model\")\n",
    "tab2_orig = tab2_orig.set_index(\"model\")\n",
    "\n",
    "# deduplicate models\n",
    "tab2_dedup = tab2[tab2_dedup_mask].sort_values(\"model\")\n",
    "tab2_dedup[\"model\"] = tab2_dedup[\"model\"].apply(lambda s: s.replace(\" (D)\", \"\"))\n",
    "tab2_dedup = tab2_dedup.set_index(\"model\")\n",
    "\n",
    "assert all(tab2_dedup.index == tab2_orig.index)\n",
    "\n",
    "print((tab2_dedup - tab2_orig).style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd2c63",
   "metadata": {},
   "source": [
    "### AuFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUFC_BASE_DIR = \"./camera_ready/table/aufc\"\n",
    "\n",
    "def print_results_aufc(data_auc, filepath):\n",
    "    table = pd.pivot(data_auc, values=[\"auc\"], index=\"model\", columns=[\"dataset_\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(table_str)\n",
    "    \n",
    "    # To latex file, leveraging rendering commands for model names\n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    print(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31021f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a file for landing page with the different metrics.\n",
    "# Ideally, we create a different json file for every dataset\n",
    "# where json file contains for every filter/max pmi constraint the models'\n",
    "# values for a given metric.\n",
    "# -----------------------\n",
    "# Example for dataset X\n",
    "# -----------------------\n",
    "# {\n",
    "#   none: {\n",
    "#     neutral__avg: {\n",
    "#        model1: 98.32,\n",
    "#        ...\n",
    "#        modeln: ...\n",
    "#     }, \n",
    "#     neutral__std: {\n",
    "#\n",
    "#     },   \n",
    "#     aufc: {\n",
    "#\n",
    "#     },  \n",
    "#     male_rel_ratio: {\n",
    "#\n",
    "#     },      \n",
    "#   },\n",
    "#   0.5: {\n",
    "#     ...\n",
    "#   },\n",
    "#   ...\n",
    "# }\n",
    "# ---------------------------------------------------------------\n",
    "METRICS_FOR_LANDING_PAGE = {name: {} for name in DATASET_NAMES}\n",
    "\n",
    "neutral__avg = {None: compute_female_male_skews(BEFORE_FILTER, MODELS)}\n",
    "neutral__std = {None: compute_neutral_pct_w_std(BEFORE_FILTER)}\n",
    "\n",
    "for eps in AFTER_FILTER.keys():\n",
    "    neutral__avg[eps] = compute_female_male_skews(AFTER_FILTER[eps], MODELS)\n",
    "    neutral__std[eps] = compute_neutral_pct_w_std(AFTER_FILTER[eps])\n",
    "\n",
    "    \n",
    "fair_auc_landing_page = {None: compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")[1]}\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    _, fair_auc = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    fair_auc_landing_page[eps] = fair_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4815a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_2_FILES,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")\n",
    "\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/unfiltered.tex\")\n",
    "\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "    print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/filter_{str(eps).replace('.', '')}.tex\")\n",
    "    fair_auc_landing_page[eps] = FAIR_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce1658-3618-4618-be4b-727d83f7071e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a63efd-aa53-4d97-9729-50b29cfc627f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
